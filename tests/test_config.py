"""
æµ‹è¯•é…ç½®æ–‡ä»¶
åŒ…å«æµ‹è¯•æ‰€éœ€çš„é…ç½®å‚æ•°å’Œè¾…åŠ©å‡½æ•°
"""

import os
from pathlib import Path
from datetime import datetime, timedelta

# é¡¹ç›®è·¯å¾„é…ç½®
PROJECT_ROOT = Path(__file__).parent.parent
TESTS_ROOT = Path(__file__).parent
DATA_ROOT = PROJECT_ROOT / "data"
LOGS_ROOT = PROJECT_ROOT / "logs"

# æµ‹è¯•æ•°æ®é…ç½®
TEST_CONFIG = {
    # æµ‹è¯•è‚¡ç¥¨ä»£ç 
    "test_symbols": [
        "000001.SZ",  # å¹³å®‰é“¶è¡Œ
        "000002.SZ",  # ä¸‡ç§‘A
        "600000.SH",  # æµ¦å‘é“¶è¡Œ
        "600036.SH",  # æ‹›å•†é“¶è¡Œ
        "000858.SZ"   # äº”ç²®æ¶²
    ],
    
    # æµ‹è¯•æ—¶é—´èŒƒå›´
    "test_date_range": {
        "start_date": "2024-01-01",
        "end_date": "2024-01-31",
        "recent_start": (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d"),
        "recent_end": (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
    },
    
    # æ•°æ®æºé…ç½®
    "data_sources": {
        "akshare": {
            "enabled": True,
            "timeout": 30,
            "retry_times": 3,
            "rate_limit": 1.0  # æ¯ç§’è¯·æ±‚é™åˆ¶
        },
        "tushare": {
            "enabled": True,
            "timeout": 30,
            "retry_times": 3,
            "token_required": True,
            "rate_limit": 0.5
        },
        "yahoo": {
            "enabled": False,  # å¯é€‰æ•°æ®æº
            "timeout": 20,
            "retry_times": 2
        }
    },
    
    # æ€§èƒ½åŸºå‡†é…ç½®
    "performance_benchmarks": {
        "max_fetch_time": 30,      # æ•°æ®è·å–æœ€å¤§æ—¶é—´(ç§’)
        "max_process_time": 10,    # æ•°æ®å¤„ç†æœ€å¤§æ—¶é—´(ç§’)
        "max_memory_mb": 500,      # æœ€å¤§å†…å­˜ä½¿ç”¨(MB)
        "max_data_rows": 10000,    # æœ€å¤§æ•°æ®è¡Œæ•°
        "min_success_rate": 0.8    # æœ€å°æˆåŠŸç‡
    },
    
    # æ•°æ®è´¨é‡é…ç½®
    "data_quality": {
        "required_columns": ["open", "high", "low", "close", "volume"],
        "optional_columns": ["amount", "turnover", "pre_close"],
        "min_data_points": 10,      # æœ€å°‘æ•°æ®ç‚¹æ•°
        "max_missing_ratio": 0.1,   # æœ€å¤§ç¼ºå¤±ç‡
        "price_change_limit": 0.15  # å•æ—¥æœ€å¤§æ¶¨è·Œå¹…é™åˆ¶
    },
    
    # ç®—æ³•æµ‹è¯•é…ç½®
    "algo_test": {
        "initial_capital": 100000,
        "benchmark": "000001.SZ",
        "test_strategies": [
            "buy_and_hold",
            "moving_average",
            "mean_reversion"
        ],
        "min_sharpe_ratio": -2.0,   # æœ€å°å¤æ™®æ¯”ç‡
        "max_drawdown": -0.5        # æœ€å¤§å›æ’¤é™åˆ¶
    },
    
    # ç¼“å­˜é…ç½®
    "cache": {
        "enabled": True,
        "ttl_seconds": 3600,        # ç¼“å­˜ç”Ÿå­˜æ—¶é—´
        "max_size_mb": 100,         # æœ€å¤§ç¼“å­˜å¤§å°
        "cleanup_interval": 300     # æ¸…ç†é—´éš”(ç§’)
    }
}

# ç¯å¢ƒå˜é‡é…ç½®
ENV_CONFIG = {
    "tushare_token": os.getenv("TUSHARE_TOKEN", ""),
    "akshare_timeout": int(os.getenv("AKSHARE_TIMEOUT", "30")),
    "test_mode": os.getenv("TEST_MODE", "unit"),  # unit, integration, full
    "log_level": os.getenv("LOG_LEVEL", "INFO"),
    "parallel_tests": os.getenv("PARALLEL_TESTS", "false").lower() == "true"
}

# æµ‹è¯•è¾…åŠ©å‡½æ•°
class TestHelpers:
    """æµ‹è¯•è¾…åŠ©å·¥å…·ç±»"""
    
    @staticmethod
    def create_test_directories():
        """åˆ›å»ºæµ‹è¯•æ‰€éœ€ç›®å½•"""
        dirs_to_create = [
            TESTS_ROOT / "temp",
            TESTS_ROOT / "fixtures",
            TESTS_ROOT / "outputs",
            DATA_ROOT / "test_cache",
            LOGS_ROOT / "tests"
        ]
        
        for dir_path in dirs_to_create:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    @staticmethod
    def cleanup_test_files():
        """æ¸…ç†æµ‹è¯•æ–‡ä»¶"""
        import shutil
        
        cleanup_dirs = [
            TESTS_ROOT / "temp",
            TESTS_ROOT / "outputs",
            DATA_ROOT / "test_cache"
        ]
        
        for dir_path in cleanup_dirs:
            if dir_path.exists():
                shutil.rmtree(dir_path)
                dir_path.mkdir(parents=True, exist_ok=True)
    
    @staticmethod
    def get_test_symbol(index: int = 0) -> str:
        """è·å–æµ‹è¯•è‚¡ç¥¨ä»£ç """
        symbols = TEST_CONFIG["test_symbols"]
        return symbols[index % len(symbols)]
    
    @staticmethod
    def get_test_date_range(recent: bool = False) -> tuple:
        """è·å–æµ‹è¯•æ—¥æœŸèŒƒå›´"""
        date_config = TEST_CONFIG["test_date_range"]
        if recent:
            return date_config["recent_start"], date_config["recent_end"]
        else:
            return date_config["start_date"], date_config["end_date"]
    
    @staticmethod
    def is_data_source_enabled(source_name: str) -> bool:
        """æ£€æŸ¥æ•°æ®æºæ˜¯å¦å¯ç”¨"""
        return TEST_CONFIG["data_sources"].get(source_name, {}).get("enabled", False)
    
    @staticmethod
    def get_performance_limit(metric: str) -> float:
        """è·å–æ€§èƒ½æŒ‡æ ‡é™åˆ¶"""
        return TEST_CONFIG["performance_benchmarks"].get(metric, float('inf'))
    
    @staticmethod
    def validate_data_quality(data, strict: bool = True) -> dict:
        """éªŒè¯æ•°æ®è´¨é‡"""
        if data is None or data.empty:
            return {"valid": False, "reason": "æ•°æ®ä¸ºç©º"}
        
        quality_config = TEST_CONFIG["data_quality"]
        issues = []
        
        # æ£€æŸ¥å¿…è¦åˆ—
        required_cols = quality_config["required_columns"]
        missing_cols = [col for col in required_cols if col not in data.columns]
        if missing_cols:
            issues.append(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
        
        # æ£€æŸ¥æ•°æ®é‡
        if len(data) < quality_config["min_data_points"]:
            issues.append(f"æ•°æ®ç‚¹ä¸è¶³: {len(data)} < {quality_config['min_data_points']}")
        
        # æ£€æŸ¥ç¼ºå¤±ç‡
        if not data.empty:
            missing_ratio = data.isnull().sum().sum() / (len(data) * len(data.columns))
            if missing_ratio > quality_config["max_missing_ratio"]:
                issues.append(f"ç¼ºå¤±ç‡è¿‡é«˜: {missing_ratio:.2%}")
        
        # æ£€æŸ¥ä»·æ ¼æ•°æ®åˆç†æ€§
        if all(col in data.columns for col in ['high', 'low', 'open', 'close']):
            price_issues = (
                (data['high'] < data['low']) |
                (data['high'] < data['open']) |
                (data['high'] < data['close']) |
                (data['low'] > data['open']) |
                (data['low'] > data['close'])
            ).sum()
            
            if price_issues > 0:
                issues.append(f"ä»·æ ¼å…³ç³»å¼‚å¸¸: {price_issues}æ¡è®°å½•")
        
        return {
            "valid": len(issues) == 0 or not strict,
            "issues": issues,
            "data_points": len(data),
            "columns": list(data.columns)
        }

# æµ‹è¯•è£…é¥°å™¨
def require_data_source(source_name: str):
    """è£…é¥°å™¨: è¦æ±‚ç‰¹å®šæ•°æ®æºå¯ç”¨"""
    def decorator(test_func):
        def wrapper(*args, **kwargs):
            if not TestHelpers.is_data_source_enabled(source_name):
                print(f"âš ï¸ è·³è¿‡æµ‹è¯• {test_func.__name__}: {source_name}æ•°æ®æºæœªå¯ç”¨")
                return
            return test_func(*args, **kwargs)
        return wrapper
    return decorator

def performance_test(max_time: float = None, max_memory_mb: float = None):
    """è£…é¥°å™¨: æ€§èƒ½æµ‹è¯•é™åˆ¶"""
    def decorator(test_func):
        def wrapper(*args, **kwargs):
            import time
            start_time = time.time()
            
            try:
                result = test_func(*args, **kwargs)
                
                elapsed = time.time() - start_time
                if max_time and elapsed > max_time:
                    raise AssertionError(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: è€—æ—¶{elapsed:.2f}s > {max_time}s")
                
                return result
                
            except Exception as e:
                elapsed = time.time() - start_time
                print(f"âš ï¸ æµ‹è¯• {test_func.__name__} å¼‚å¸¸ (è€—æ—¶{elapsed:.2f}s): {e}")
                raise
        return wrapper
    return decorator

# åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ
def setup_test_environment():
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
    # åˆ›å»ºå¿…è¦ç›®å½•
    TestHelpers.create_test_directories()
    
    # è®¾ç½®æ—¥å¿—
    import logging
    log_level = getattr(logging, ENV_CONFIG["log_level"].upper(), logging.INFO)
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(LOGS_ROOT / "tests" / "test.log"),
            logging.StreamHandler()
        ]
    )
    
    print(f"âœ… æµ‹è¯•ç¯å¢ƒå·²åˆå§‹åŒ–")
    print(f"   é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
    print(f"   æµ‹è¯•ç›®å½•: {TESTS_ROOT}")
    print(f"   æ—¥å¿—çº§åˆ«: {ENV_CONFIG['log_level']}")
    print(f"   æµ‹è¯•æ¨¡å¼: {ENV_CONFIG['test_mode']}")

def teardown_test_environment():
    """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
    TestHelpers.cleanup_test_files()
    print("âœ… æµ‹è¯•ç¯å¢ƒå·²æ¸…ç†")

if __name__ == "__main__":
    # æµ‹è¯•é…ç½®æ–‡ä»¶
    setup_test_environment()
    
    print("\nğŸ“‹ æµ‹è¯•é…ç½®:")
    print(f"   æµ‹è¯•è‚¡ç¥¨: {TEST_CONFIG['test_symbols']}")
    print(f"   æ—¶é—´èŒƒå›´: {TEST_CONFIG['test_date_range']}")
    print(f"   å¯ç”¨çš„æ•°æ®æº: {[k for k, v in TEST_CONFIG['data_sources'].items() if v['enabled']]}")
    
    teardown_test_environment()