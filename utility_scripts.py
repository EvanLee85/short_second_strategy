# =============================================================================
# zipline_ingest.py - Ziplineæ•°æ®æ‘„å…¥
# =============================================================================

ZIPLINE_INGEST_SCRIPT = '''#!/usr/bin/env python3
"""
Ziplineæ•°æ®æ‘„å…¥å·¥å…·
å°†CSVæ•°æ®æ‘„å…¥åˆ°Ziplineä¸­ç”¨äºå›æµ‹

ä½¿ç”¨æ–¹å¼:
python scripts/zipline_ingest.py --bundle custom_bundle
python scripts/zipline_ingest.py --bundle custom_bundle --symbols 000001.SZ,600000.SH
"""

import sys
import subprocess
import argparse
from pathlib import Path
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

def setup_zipline_environment():
    """è®¾ç½®Ziplineç¯å¢ƒå˜é‡"""
    # è®¾ç½®Ziplineæ ¹ç›®å½•
    zipline_root = PROJECT_ROOT / 'data' / 'zipline'
    zipline_root.mkdir(parents=True, exist_ok=True)#!/usr/bin/env python3
"""
å®ç”¨å·¥å…·è„šæœ¬é›†åˆ
åŒ…å«æ–‡æ¡£ä¸­æåˆ°çš„æ‰€æœ‰å·¥å…·è„šæœ¬çš„å®é™…å®ç°

æ–‡ä»¶ç»“æ„:
- verify_token.py - éªŒè¯API tokenæœ‰æ•ˆæ€§
- clear_cache.py - æ¸…ç†å„ç§ç¼“å­˜
- inspect_raw_data.py - æ£€æŸ¥åŸå§‹æ•°æ®
- validate_data_format.py - éªŒè¯æ•°æ®æ ¼å¼
- verify_adjustment.py - éªŒè¯å¤æƒä¸€è‡´æ€§
- check_price_relationships.py - æ£€æŸ¥ä»·æ ¼å…³ç³»
- fix_price_relationships.py - ä¿®å¤ä»·æ ¼å…³ç³»å¼‚å¸¸
- align_trading_calendar.py - å¯¹é½äº¤æ˜“æ—¥å†
- validate_zipline_format.py - éªŒè¯Ziplineæ ¼å¼
- zipline_ingest.py - Ziplineæ•°æ®æ‘„å…¥
- test_zipline_data.py - æµ‹è¯•Ziplineæ•°æ®
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# verify_token.py - éªŒè¯API tokenæœ‰æ•ˆæ€§
# =============================================================================

VERIFY_TOKEN_SCRIPT = '''#!/usr/bin/env python3
"""
API TokenéªŒè¯å·¥å…·
éªŒè¯å„ç§æ•°æ®æºAPI tokençš„æœ‰æ•ˆæ€§

ä½¿ç”¨æ–¹å¼:
python scripts/verify_token.py --source tushare
python scripts/verify_token.py --source akshare
python scripts/verify_token.py --all
"""

import sys
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

def verify_tushare_token():
    """éªŒè¯Tushare token"""
    try:
        import tushare as ts
        from config.settings import DATA_SOURCES
        
        token = DATA_SOURCES.get('tushare', {}).get('token')
        if not token:
            print("âŒ Tushare tokenæœªé…ç½®")
            return False
        
        # è®¾ç½®tokenå¹¶æµ‹è¯•
        ts.set_token(token)
        pro = ts.pro_api()
        
        # æµ‹è¯•APIè°ƒç”¨
        df = pro.stock_basic(exchange='', list_status='L', fields='ts_code,symbol,name')
        
        if not df.empty:
            print(f"âœ… Tushare tokenéªŒè¯æˆåŠŸ")
            print(f"   è·å–åˆ° {len(df)} åªè‚¡ç¥¨ä¿¡æ¯")
            return True
        else:
            print("âŒ Tushare tokenéªŒè¯å¤±è´¥: è¿”å›ç©ºæ•°æ®")
            return False
            
    except ImportError:
        print("âš ï¸  Tushareæœªå®‰è£…ï¼Œè·³è¿‡éªŒè¯")
        return None
    except Exception as e:
        print(f"âŒ Tushare tokenéªŒè¯å¤±è´¥: {e}")
        return False

def verify_akshare_connection():
    """éªŒè¯Akshareè¿æ¥"""
    try:
        import akshare as ak
        
        # æµ‹è¯•è·å–è‚¡ç¥¨åˆ—è¡¨
        df = ak.stock_zh_a_spot_em()
        
        if not df.empty:
            print(f"âœ… Akshareè¿æ¥éªŒè¯æˆåŠŸ")
            print(f"   è·å–åˆ° {len(df)} åªè‚¡ç¥¨ä¿¡æ¯")
            return True
        else:
            print("âŒ Akshareè¿æ¥éªŒè¯å¤±è´¥: è¿”å›ç©ºæ•°æ®")
            return False
            
    except ImportError:
        print("âš ï¸  Akshareæœªå®‰è£…ï¼Œè·³è¿‡éªŒè¯")
        return None
    except Exception as e:
        print(f"âŒ Akshareè¿æ¥éªŒè¯å¤±è´¥: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='API TokenéªŒè¯å·¥å…·')
    parser.add_argument('--source', choices=['tushare', 'akshare'], help='æŒ‡å®šæ•°æ®æº')
    parser.add_argument('--all', action='store_true', help='éªŒè¯æ‰€æœ‰æ•°æ®æº')
    
    args = parser.parse_args()
    
    print("ğŸ” API TokenéªŒè¯å·¥å…·")
    print("=" * 40)
    
    results = {}
    
    if args.source == 'tushare' or args.all:
        results['tushare'] = verify_tushare_token()
    
    if args.source == 'akshare' or args.all:
        results['akshare'] = verify_akshare_connection()
    
    if not args.source and not args.all:
        # é»˜è®¤éªŒè¯æ‰€æœ‰
        results['tushare'] = verify_tushare_token()
        results['akshare'] = verify_akshare_connection()
    
    # æ±‡æ€»ç»“æœ
    print("\\nğŸ“Š éªŒè¯ç»“æœæ±‡æ€»:")
    success_count = 0
    total_count = 0
    
    for source, result in results.items():
        if result is not None:
            total_count += 1
            if result:
                success_count += 1
                print(f"   âœ… {source}: é€šè¿‡")
            else:
                print(f"   âŒ {source}: å¤±è´¥")
        else:
            print(f"   âš ï¸  {source}: è·³è¿‡")
    
    if total_count > 0:
        print(f"\\næˆåŠŸç‡: {success_count}/{total_count}")
        return 0 if success_count == total_count else 1
    else:
        print("\\næ²¡æœ‰å¯éªŒè¯çš„æ•°æ®æº")
        return 0

if __name__ == "__main__":
    sys.exit(main())
'''

# =============================================================================
# clear_cache.py - æ¸…ç†å„ç§ç¼“å­˜
# =============================================================================

CLEAR_CACHE_SCRIPT = '''#!/usr/bin/env python3
"""
ç¼“å­˜æ¸…ç†å·¥å…·
æ¸…ç†é¡¹ç›®ä¸­çš„å„ç§ç¼“å­˜æ•°æ®

ä½¿ç”¨æ–¹å¼:
python scripts/clear_cache.py --type all
python scripts/clear_cache.py --type data
python scripts/clear_cache.py --type adjustment
python scripts/clear_cache.py --type auth
"""

import sys
import shutil
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

def clear_data_cache():
    """æ¸…ç†æ•°æ®ç¼“å­˜"""
    cache_dirs = [
        PROJECT_ROOT / 'data' / 'cache',
        PROJECT_ROOT / 'data' / 'temp',
        PROJECT_ROOT / '.cache'
    ]
    
    cleared_count = 0
    total_size = 0
    
    for cache_dir in cache_dirs:
        if cache_dir.exists():
            # è®¡ç®—å¤§å°
            size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())
            total_size += size
            
            # åˆ é™¤å†…å®¹
            shutil.rmtree(cache_dir)
            cache_dir.mkdir(parents=True, exist_ok=True)
            
            print(f"âœ… æ¸…ç†æ•°æ®ç¼“å­˜: {cache_dir} ({size/1024/1024:.1f} MB)")
            cleared_count += 1
    
    return cleared_count, total_size

def clear_adjustment_cache():
    """æ¸…ç†å¤æƒå› å­ç¼“å­˜"""
    adjustment_files = [
        PROJECT_ROOT / 'data' / 'adjustment_factors.pkl',
        PROJECT_ROOT / 'data' / 'adjustment_factors.json',
    ]
    
    cleared_count = 0
    
    for adj_file in adjustment_files:
        if adj_file.exists():
            size = adj_file.stat().st_size
            adj_file.unlink()
            print(f"âœ… æ¸…ç†å¤æƒç¼“å­˜: {adj_file} ({size/1024:.1f} KB)")
            cleared_count += 1
    
    return cleared_count

def clear_auth_cache():
    """æ¸…ç†è®¤è¯ç¼“å­˜"""
    auth_files = [
        PROJECT_ROOT / '.tushare_cache',
        PROJECT_ROOT / '.auth_cache',
        Path.home() / '.tushare' / 'token_cache'
    ]
    
    cleared_count = 0
    
    for auth_file in auth_files:
        if auth_file.exists():
            if auth_file.is_dir():
                shutil.rmtree(auth_file)
            else:
                auth_file.unlink()
            print(f"âœ… æ¸…ç†è®¤è¯ç¼“å­˜: {auth_file}")
            cleared_count += 1
    
    return cleared_count

def clear_log_files():
    """æ¸…ç†æ—¥å¿—æ–‡ä»¶"""
    log_dirs = [
        PROJECT_ROOT / 'logs',
        PROJECT_ROOT / 'log'
    ]
    
    cleared_count = 0
    total_size = 0
    
    for log_dir in log_dirs:
        if log_dir.exists():
            for log_file in log_dir.glob('*.log'):
                if log_file.stat().st_size > 10 * 1024 * 1024:  # å¤§äº10MB
                    size = log_file.stat().st_size
                    log_file.unlink()
                    total_size += size
                    print(f"âœ… æ¸…ç†å¤§æ—¥å¿—æ–‡ä»¶: {log_file} ({size/1024/1024:.1f} MB)")
                    cleared_count += 1
    
    return cleared_count, total_size

def clear_temp_files():
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    temp_patterns = [
        '**/.DS_Store',
        '**/__pycache__',
        '**/*.pyc',
        '**/*.pyo',
        '**/temp_*',
        '**/tmp_*'
    ]
    
    cleared_count = 0
    
    for pattern in temp_patterns:
        for temp_file in PROJECT_ROOT.glob(pattern):
            if temp_file.is_file():
                temp_file.unlink()
                cleared_count += 1
            elif temp_file.is_dir():
                shutil.rmtree(temp_file)
                cleared_count += 1
    
    if cleared_count > 0:
        print(f"âœ… æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {cleared_count} ä¸ª")
    
    return cleared_count

def main():
    parser = argparse.ArgumentParser(description='ç¼“å­˜æ¸…ç†å·¥å…·')
    parser.add_argument('--type', 
                       choices=['all', 'data', 'adjustment', 'auth', 'logs', 'temp'],
                       default='all',
                       help='æŒ‡å®šæ¸…ç†ç±»å‹')
    parser.add_argument('--dry-run', action='store_true', help='è¯•è¿è¡Œï¼Œä¸å®é™…åˆ é™¤')
    
    args = parser.parse_args()
    
    print("ğŸ§¹ ç¼“å­˜æ¸…ç†å·¥å…·")
    print("=" * 40)
    
    if args.dry_run:
        print("âš ï¸  è¯•è¿è¡Œæ¨¡å¼ï¼Œä¸ä¼šå®é™…åˆ é™¤æ–‡ä»¶")
    
    total_cleared = 0
    total_size = 0
    
    if args.type in ['all', 'data']:
        count, size = clear_data_cache()
        total_cleared += count
        total_size += size
    
    if args.type in ['all', 'adjustment']:
        count = clear_adjustment_cache()
        total_cleared += count
    
    if args.type in ['all', 'auth']:
        count = clear_auth_cache()
        total_cleared += count
    
    if args.type in ['all', 'logs']:
        count, size = clear_log_files()
        total_cleared += count
        total_size += size
    
    if args.type in ['all', 'temp']:
        count = clear_temp_files()
        total_cleared += count
    
    print(f"\\nğŸ“Š æ¸…ç†å®Œæˆ:")
    print(f"   æ¸…ç†é¡¹ç›®: {total_cleared}")
    print(f"   é‡Šæ”¾ç©ºé—´: {total_size/1024/1024:.1f} MB")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
'''

# =============================================================================
# inspect_raw_data.py - æ£€æŸ¥åŸå§‹æ•°æ®
# =============================================================================

INSPECT_RAW_DATA_SCRIPT = '''#!/usr/bin/env python3
"""
åŸå§‹æ•°æ®æ£€æŸ¥å·¥å…·
æ£€æŸ¥å’Œåˆ†æåŸå§‹è‚¡ç¥¨æ•°æ®çš„è´¨é‡å’Œç»“æ„

ä½¿ç”¨æ–¹å¼:
python scripts/inspect_raw_data.py --symbol 000001.SZ --source akshare
python scripts/inspect_raw_data.py --file data/raw/000001.SZ.csv
"""

import sys
import pandas as pd
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

def inspect_dataframe(df, symbol_name="Unknown"):
    """æ£€æŸ¥DataFrameçš„è¯¦ç»†ä¿¡æ¯"""
    
    print(f"ğŸ“Š æ•°æ®æ£€æŸ¥æŠ¥å‘Š: {symbol_name}")
    print("=" * 50)
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"æ•°æ®ç»´åº¦: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
    print(f"å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024:.1f} KB")
    
    # åˆ—ä¿¡æ¯
    print(f"\\nğŸ“‹ åˆ—ä¿¡æ¯:")
    for i, (col, dtype) in enumerate(zip(df.columns, df.dtypes)):
        null_count = df[col].isnull().sum()
        null_pct = null_count / len(df) * 100
        print(f"   {i+1:2d}. {col:15s} ({str(dtype):10s}) - ç¼ºå¤±: {null_count:4d} ({null_pct:5.1f}%)")
    
    # æ•°å€¼åˆ—ç»Ÿè®¡
    numeric_cols = df.select_dtypes(include=[float, int]).columns
    if len(numeric_cols) > 0:
        print(f"\\nğŸ“ˆ æ•°å€¼åˆ—ç»Ÿè®¡:")
        print(df[numeric_cols].describe().round(4))
    
    # ä»·æ ¼å…³ç³»æ£€æŸ¥
    price_cols = ['open', 'high', 'low', 'close']
    available_price_cols = [col for col in price_cols if col in df.columns]
    
    if len(available_price_cols) >= 4:
        print(f"\\nğŸ’° ä»·æ ¼å…³ç³»æ£€æŸ¥:")
        
        # high >= low
        high_low_ok = (df['high'] >= df['low']).sum()
        print(f"   high >= low: {high_low_ok:4d}/{len(df)} ({high_low_ok/len(df)*100:5.1f}%)")
        
        # high >= open, close
        high_open_ok = (df['high'] >= df['open']).sum()
        high_close_ok = (df['high'] >= df['close']).sum()
        print(f"   high >= open: {high_open_ok:4d}/{len(df)} ({high_open_ok/len(df)*100:5.1f}%)")
        print(f"   high >= close: {high_close_ok:4d}/{len(df)} ({high_close_ok/len(df)*100:5.1f}%)")
        
        # low <= open, close
        low_open_ok = (df['low'] <= df['open']).sum()
        low_close_ok = (df['low'] <= df['close']).sum()
        print(f"   low <= open: {low_open_ok:4d}/{len(df)} ({low_open_ok/len(df)*100:5.1f}%)")
        print(f"   low <= close: {low_close_ok:4d}/{len(df)} ({low_close_ok/len(df)*100:5.1f}%)")
    
    # æˆäº¤é‡æ£€æŸ¥
    if 'volume' in df.columns:
        print(f"\\nğŸ“Š æˆäº¤é‡æ£€æŸ¥:")
        vol_positive = (df['volume'] > 0).sum()
        vol_zero = (df['volume'] == 0).sum()
        vol_negative = (df['volume'] < 0).sum()
        
        print(f"   æ­£æ•°æˆäº¤é‡: {vol_positive:4d} ({vol_positive/len(df)*100:5.1f}%)")
        print(f"   é›¶æˆäº¤é‡: {vol_zero:4d} ({vol_zero/len(df)*100:5.1f}%)")
        print(f"   è´Ÿæ•°æˆäº¤é‡: {vol_negative:4d} ({vol_negative/len(df)*100:5.1f}%)")
    
    # æ—¥æœŸè¿ç»­æ€§æ£€æŸ¥
    date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]
    if date_cols:
        date_col = date_cols[0]
        print(f"\\nğŸ“… æ—¥æœŸè¿ç»­æ€§æ£€æŸ¥ (åˆ—: {date_col}):")
        
        try:
            df_sorted = df.copy()
            df_sorted[date_col] = pd.to_datetime(df_sorted[date_col])
            df_sorted = df_sorted.sort_values(date_col)
            
            date_range = df_sorted[date_col].max() - df_sorted[date_col].min()
            print(f"   æ—¶é—´è·¨åº¦: {date_range.days} å¤©")
            print(f"   å¼€å§‹æ—¥æœŸ: {df_sorted[date_col].min().strftime('%Y-%m-%d')}")
            print(f"   ç»“æŸæ—¥æœŸ: {df_sorted[date_col].max().strftime('%Y-%m-%d')}")
            
            # æ£€æŸ¥é‡å¤æ—¥æœŸ
            duplicates = df_sorted[date_col].duplicated().sum()
            if duplicates > 0:
                print(f"   âš ï¸  é‡å¤æ—¥æœŸ: {duplicates} ä¸ª")
            
        except Exception as e:
            print(f"   âŒ æ—¥æœŸæ£€æŸ¥å¤±è´¥: {e}")
    
    # å¼‚å¸¸å€¼æ£€æŸ¥
    if len(available_price_cols) > 0:
        print(f"\\nâš ï¸  å¼‚å¸¸å€¼æ£€æŸ¥:")
        
        for col in available_price_cols:
            q1 = df[col].quantile(0.25)
            q3 = df[col].quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
            print(f"   {col:10s}: {outliers:4d} ä¸ªå¼‚å¸¸å€¼ ({outliers/len(df)*100:5.1f}%)")
    
    # æ•°æ®æ ·ä¾‹
    print(f"\\nğŸ“ æ•°æ®æ ·ä¾‹ (å‰5è¡Œ):")
    print(df.head().to_string())
    
    if len(df) > 10:
        print(f"\\nğŸ“ æ•°æ®æ ·ä¾‹ (å5è¡Œ):")
        print(df.tail().to_string())

def inspect_from_source(symbol, source):
    """ä»æ•°æ®æºç›´æ¥è·å–æ•°æ®å¹¶æ£€æŸ¥"""
    try:
        from data_sources.unified_fetcher import UnifiedDataFetcher
        
        fetcher = UnifiedDataFetcher()
        df = fetcher.get_stock_data(
            symbol=symbol,
            start_date='2024-01-01',
            end_date='2024-01-31',
            source=source
        )
        
        if df.empty:
            print(f"âŒ ä» {source} è·å– {symbol} çš„æ•°æ®ä¸ºç©º")
            return False
        
        inspect_dataframe(df, f"{symbol} (æ¥æº: {source})")
        return True
        
    except Exception as e:
        print(f"âŒ ä» {source} è·å– {symbol} æ•°æ®å¤±è´¥: {e}")
        return False

def inspect_from_file(file_path):
    """ä»æ–‡ä»¶è¯»å–æ•°æ®å¹¶æ£€æŸ¥"""
    try:
        file_path = Path(file_path)
        
        if not file_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
        
        # å°è¯•ä¸åŒçš„è¯»å–æ–¹å¼
        try:
            df = pd.read_csv(file_path)
        except Exception as e1:
            try:
                df = pd.read_csv(file_path, encoding='gbk')
            except Exception as e2:
                print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e1}")
                return False
        
        if df.empty:
            print(f"âŒ æ–‡ä»¶æ•°æ®ä¸ºç©º: {file_path}")
            return False
        
        inspect_dataframe(df, file_path.name)
        return True
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥æ–‡ä»¶å¤±è´¥: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='åŸå§‹æ•°æ®æ£€æŸ¥å·¥å…·')
    parser.add_argument('--symbol', help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--source', help='æ•°æ®æº')
    parser.add_argument('--file', help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    print("ğŸ” åŸå§‹æ•°æ®æ£€æŸ¥å·¥å…·")
    print("=" * 50)
    
    if args.file:
        success = inspect_from_file(args.file)
    elif args.symbol and args.source:
        success = inspect_from_source(args.symbol, args.source)
    elif args.symbol:
        # é»˜è®¤ä½¿ç”¨ç»Ÿä¸€è·å–å™¨
        success = inspect_from_source(args.symbol, None)
    else:
        print("âŒ è¯·æŒ‡å®š --symbol å’Œ --sourceï¼Œæˆ–è€… --file")
        return 1
    
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())
'''

# =============================================================================
# validate_data_format.py - éªŒè¯æ•°æ®æ ¼å¼
# =============================================================================

VALIDATE_DATA_FORMAT_SCRIPT = '''#!/usr/bin/env python3
"""
æ•°æ®æ ¼å¼éªŒè¯å·¥å…·
éªŒè¯CSVæ•°æ®æ–‡ä»¶æ˜¯å¦ç¬¦åˆç³»ç»Ÿè¦æ±‚çš„æ ¼å¼

ä½¿ç”¨æ–¹å¼:
python scripts/validate_data_format.py --input data/raw/000001.SZ.csv
python scripts/validate_data_format.py --input data/raw/ --recursive
"""

import sys
import pandas as pd
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# æ ‡å‡†æ•°æ®æ ¼å¼è¦æ±‚
REQUIRED_COLUMNS = ['date', 'open', 'high', 'low', 'close', 'volume']
OPTIONAL_COLUMNS = ['pre_close', 'change', 'pct_change', 'amount', 'turnover']
COLUMN_TYPES = {
    'date': 'datetime',
    'open': 'float',
    'high': 'float', 
    'low': 'float',
    'close': 'float',
    'volume': 'int',
    'pre_close': 'float',
    'amount': 'float'
}

def validate_single_file(file_path):
    """éªŒè¯å•ä¸ªæ–‡ä»¶"""
    file_path = Path(file_path)
    
    if not file_path.exists():
        return {
            'file': str(file_path),
            'valid': False,
            'error': 'æ–‡ä»¶ä¸å­˜åœ¨',
            'issues': []
        }
    
    try:
        # è¯»å–æ–‡ä»¶
        df = pd.read_csv(file_path)
        
        issues = []
        warnings = []
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if df.empty:
            issues.append('æ–‡ä»¶ä¸ºç©º')
            return {
                'file': str(file_path),
                'valid': False,
                'error': 'æ–‡ä»¶ä¸ºç©º',
                'issues': issues
            }
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        missing_required = [col for col in REQUIRED_COLUMNS if col not in df.columns]
        if missing_required:
            issues.append(f'ç¼ºå°‘å¿…éœ€åˆ—: {missing_required}')
        
        # æ£€æŸ¥åˆ—åè§„èŒƒ
        invalid_columns = []
        for col in df.columns:
            if col not in REQUIRED_COLUMNS + OPTIONAL_COLUMNS:
                invalid_columns.append(col)
        
        if invalid_columns:
            warnings.append(f'æœªè¯†åˆ«çš„åˆ—: {invalid_columns}')
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        for col, expected_type in COLUMN_TYPES.items():
            if col in df.columns:
                if expected_type == 'datetime':
                    try:
                        pd.to_datetime(df[col])
                    except:
                        issues.append(f'åˆ— {col} ä¸æ˜¯æœ‰æ•ˆçš„æ—¥æœŸæ ¼å¼')
                elif expected_type == 'float':
                    if not pd.api.types.is_numeric_dtype(df[col]):
                        issues.append(f'åˆ— {col} ä¸æ˜¯æ•°å€¼ç±»å‹')
                elif expected_type == 'int':
                    if not pd.api.types.is_numeric_dtype(df[col]):
                        issues.append(f'åˆ— {col} ä¸æ˜¯æ•°å€¼ç±»å‹')
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        total_cells = len(df) * len(df.columns)
        missing_cells = df.isnull().sum().sum()
        missing_ratio = missing_cells / total_cells
        
        if missing_ratio > 0.1:  # è¶…è¿‡10%ç¼ºå¤±
            issues.append(f'ç¼ºå¤±æ•°æ®è¿‡å¤š: {missing_ratio:.1%}')
        elif missing_ratio > 0.05:  # è¶…è¿‡5%è­¦å‘Š
            warnings.append(f'å­˜åœ¨ç¼ºå¤±æ•°æ®: {missing_ratio:.1%}')
        
        # æ£€æŸ¥ä»·æ ¼å…³ç³»
        price_cols = ['open', 'high', 'low', 'close']
        if all(col in df.columns for col in price_cols):
            # high >= low
            invalid_high_low = (df['high'] < df['low']).sum()
            if invalid_high_low > 0:
                issues.append(f'{invalid_high_low} è¡Œæ•°æ® high < low')
            
            # æ£€æŸ¥è´Ÿä»·æ ¼
            negative_prices = 0
            for col in price_cols:
                negative_prices += (df[col] <= 0).sum()
            
            if negative_prices > 0:
                issues.append(f'{negative_prices} ä¸ªéæ­£ä»·æ ¼æ•°æ®')
        
        # æ£€æŸ¥æˆäº¤é‡
        if 'volume' in df.columns:
            negative_volume = (df['volume'] < 0).sum()
            if negative_volume > 0:
                issues.append(f'{negative_volume} è¡Œè´Ÿæˆäº¤é‡')
        
        # æ£€æŸ¥æ—¥æœŸè¿ç»­æ€§
        if 'date' in df.columns:
            try:
                df_sorted = df.copy()
                df_sorted['date'] = pd.to_datetime(df_sorted['date'])
                df_sorted = df_sorted.sort_values('date')
                
                # æ£€æŸ¥é‡å¤æ—¥æœŸ
                duplicates = df_sorted['date'].duplicated().sum()
                if duplicates > 0:
                    issues.append(f'{duplicates} ä¸ªé‡å¤æ—¥æœŸ')
                
            except:
                issues.append('æ—¥æœŸæ ¼å¼æ— æ³•è§£æ')
        
        return {
            'file': str(file_path),
            'valid': len(issues) == 0,
            'issues': issues,
            'warnings': warnings,
            'rows': len(df),
            'columns': list(df.columns),
            'missing_ratio': missing_ratio
        }
        
    except Exception as e:
        return {
            'file': str(file_path),
            'valid': False,
            'error': str(e),
            'issues': [f'æ–‡ä»¶è¯»å–å¤±è´¥: {e}']
        }

def validate_directory(dir_path, recursive=False):
    """éªŒè¯ç›®å½•ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶"""
    dir_path = Path(dir_path)
    
    if recursive:
        csv_files = list(dir_path.rglob("*.csv"))
    else:
        csv_files = list(dir_path.glob("*.csv"))
    
    results = []
    
    print(f"ğŸ” æ‰«æç›®å½•: {dir_path}")
    print(f"   æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    for csv_file in csv_files:
        print(f"   éªŒè¯: {csv_file.name}")
        result = validate_single_file(csv_file)
        results.append(result)
    
    return results

def print_validation_results(results):
    """æ‰“å°éªŒè¯ç»“æœ"""
    valid_count = sum(1 for r in results if r['valid'])
    total_count = len(results)
    
    print(f"\\nğŸ“Š éªŒè¯ç»“æœæ±‡æ€»:")
    print(f"   æ€»æ–‡ä»¶æ•°: {total_count}")
    print(f"   æœ‰æ•ˆæ–‡ä»¶: {valid_count}")
    print(f"   æ— æ•ˆæ–‡ä»¶: {total_count - valid_count}")
    print(f"   é€šè¿‡ç‡: {valid_count/total_count*100:.1f}%")
    
    # è¯¦ç»†ç»“æœ
    print(f"\\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    
    for result in results:
        file_name = Path(result['file']).name
        
        if result['valid']:
            print(f"   âœ… {file_name}")
            print(f"      è¡Œæ•°: {result['rows']}, åˆ—æ•°: {len(result['columns'])}")
            if result.get('warnings'):
                for warning in result['warnings']:
                    print(f"      âš ï¸  {warning}")
        else:
            print(f"   âŒ {file_name}")
            if 'error' in result:
                print(f"      é”™è¯¯: {result['error']}")
            
            for issue in result['issues']:
                print(f"      é—®é¢˜: {issue}")

def main():
    parser = argparse.ArgumentParser(description='æ•°æ®æ ¼å¼éªŒè¯å·¥å…·')
    parser.add_argument('--input', required=True, help='è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--recursive', action='store_true', help='é€’å½’æ‰«æå­ç›®å½•')
    parser.add_argument('--output', help='è¾“å‡ºéªŒè¯æŠ¥å‘Šåˆ°JSONæ–‡ä»¶')
    
    args = parser.parse_args()
    
    print("âœ… æ•°æ®æ ¼å¼éªŒè¯å·¥å…·")
    print("=" * 40)
    
    input_path = Path(args.input)
    
    if not input_path.exists():
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return 1
    
    if input_path.is_file():
        results = [validate_single_file(input_path)]
    elif input_path.is_dir():
        results = validate_directory(input_path, args.recursive)
    else:
        print(f"âŒ æ— æ•ˆè·¯å¾„ç±»å‹: {input_path}")
        return 1
    
    print_validation_results(results)
    
    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        import json
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        print(f"\\nğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {args.output}")
    
    # è¿”å›çŠ¶æ€ç 
    valid_count = sum(1 for r in results if r['valid'])
    return 0 if valid_count == len(results) else 1

if __name__ == "__main__":
    sys.exit(main())
'''

# =============================================================================
# verify_adjustment.py - éªŒè¯å¤æƒä¸€è‡´æ€§
# =============================================================================

VERIFY_ADJUSTMENT_SCRIPT = '''#!/usr/bin/env python3
"""
å¤æƒä¸€è‡´æ€§éªŒè¯å·¥å…·
æ£€æŸ¥è‚¡ç¥¨åœ¨ä¸åŒæ—¶é—´è·å–çš„å¤æƒæ•°æ®æ˜¯å¦ä¸€è‡´

ä½¿ç”¨æ–¹å¼:
python scripts/verify_adjustment.py --symbol 000001.SZ --dates 2024-01-01,2024-06-01,2024-12-01
python scripts/verify_adjustment.py --symbol 000001.SZ --check-consistency
"""

import sys
import pandas as pd
import argparse
from pathlib import Path
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

def fetch_data_at_different_times(symbol, base_period, fetch_dates):
    """åœ¨ä¸åŒæ—¶é—´ç‚¹è·å–ç›¸åŒå†å²æœŸé—´çš„æ•°æ®"""
    try:
        from data_sources.unified_fetcher import UnifiedDataFetcher
        
        fetcher = UnifiedDataFetcher()
        results = {}
        
        start_date, end_date = base_period
        
        print(f"ğŸ“Š è·å– {symbol} åœ¨ {start_date} ~ {end_date} æœŸé—´çš„æ•°æ®")
        print(f"   æ¨¡æ‹Ÿåœ¨ä¸åŒæ—¥æœŸè·å–æ•°æ®: {fetch_dates}")
        
        for fetch_date in fetch_dates:
            print(f"\\nğŸ“„ æ¨¡æ‹Ÿåœ¨ {fetch_date} è·å–æ•°æ®...")
            
            # è¿™é‡Œæ¨¡æ‹Ÿåœ¨ä¸åŒæ—¶é—´è·å–æ•°æ®
            # å®é™…ä¸­å¯èƒ½éœ€è¦è°ƒæ•´å¤æƒåŸºå‡†æ—¥æœŸ
            data = fetcher.get_stock_data(
                symbol=symbol,
                start_date=start_date,
                end_date=end_date,
                adjust='qfq'  # å‰å¤æƒ
            )
            
            if not data.empty:
                results[fetch_date] = data
                print(f"   âœ… è·å–æˆåŠŸ: {len(data)} è¡Œæ•°æ®")
                print(f"   ä»·æ ¼èŒƒå›´: {data['close'].min():.3f} ~ {data['close'].max():.3f}")
            else:
                print(f"   âŒ è·å–å¤±è´¥: æ•°æ®ä¸ºç©º")
        
        return results
        
    except Exception as e:
        print(f"âŒ æ•°æ®è·å–å¤±è´¥: {e}")
        return {}

def compare_adjustment_consistency(data_dict):
    """æ¯”è¾ƒå¤æƒæ•°æ®ä¸€è‡´æ€§"""
    if len(data_dict) < 2:
        print("âŒ éœ€è¦è‡³å°‘ä¸¤ä¸ªæ—¶é—´ç‚¹çš„æ•°æ®è¿›è¡Œæ¯”è¾ƒ")
        return False
    
    print(f"\\nğŸ” å¤æƒä¸€è‡´æ€§åˆ†æ:")
    print("=" * 50)
    
    # è·å–æ‰€æœ‰æ•°æ®çš„æ—¥æœŸäº¤é›†
    date_sets = []
    for fetch_date, data in data_dict.items():
        if 'date' in data.columns:
            dates = set(pd.to_datetime(data['date']).dt.date)
            date_sets.append(dates)
    
    if not date_sets:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ—¥æœŸåˆ—")
        return False
    
    common_dates = date_sets[0]
    for date_set in date_sets[1:]:
        common_dates &= date_set
    
    print(f"ğŸ“… å…±åŒäº¤æ˜“æ—¥æ•°é‡: {len(common_dates)}")
    
    if len(common_dates) == 0:
        print("âŒ æ²¡æœ‰å…±åŒçš„äº¤æ˜“æ—¥æœŸ")
        return False
    
    # æŒ‰æ—¥æœŸå¯¹é½æ•°æ®
    aligned_data = {}
    for fetch_date, data in data_dict.items():
        data_copy = data.copy()
        data_copy['date'] = pd.to_datetime(data_copy['date'])
        data_copy = data_copy[data_copy['date'].dt.date.isin(common_dates)]
        data_copy = data_copy.sort_values('date').reset_index(drop=True)
        aligned_data[fetch_date] = data_copy
    
    # æ¯”è¾ƒä»·æ ¼ä¸€è‡´æ€§
    fetch_dates = list(aligned_data.keys())
    base_data = aligned_data[fetch_dates[0]]
    
    print(f"\\nğŸ’° ä»·æ ¼ä¸€è‡´æ€§æ£€æŸ¥:")
    print(f"   åŸºå‡†æ—¶é—´: {fetch_dates[0]}")
    
    price_columns = ['open', 'high', 'low', 'close']
    consistency_results = {}
    
    for price_col in price_columns:
        if price_col not in base_data.columns:
            continue
            
        max_diff = 0
        max_diff_pct = 0
        
        print(f"\\n   {price_col.upper()} åˆ—æ¯”è¾ƒ:")
        
        for i, compare_date in enumerate(fetch_dates[1:], 1):
            compare_data = aligned_data[compare_date]
            
            if price_col not in compare_data.columns:
                print(f"      vs {compare_date}: âŒ åˆ—ä¸å­˜åœ¨")
                continue
            
            # è®¡ç®—å·®å¼‚
            diff = abs(base_data[price_col] - compare_data[price_col])
            diff_pct = abs(diff / base_data[price_col] * 100)
            
            max_single_diff = diff.max()
            max_single_diff_pct = diff_pct.max()
            mean_diff = diff.mean()
            mean_diff_pct = diff_pct.mean()
            
            max_diff = max(max_diff, max_single_diff)
            max_diff_pct = max(max_diff_pct, max_single_diff_pct)
            
            print(f"      vs {compare_date}:")
            print(f"         æœ€å¤§å·®å¼‚: {max_single_diff:.6f} ({max_single_diff_pct:.4f}%)")
            print(f"         å¹³å‡å·®å¼‚: {mean_diff:.6f} ({mean_diff_pct:.4f}%)")
            
            # åˆ¤æ–­ä¸€è‡´æ€§
            if max_single_diff_pct < 0.01:  # 0.01%ä»¥å†…è®¤ä¸ºä¸€è‡´
                print(f"         ç»“æœ: âœ… é«˜åº¦ä¸€è‡´")
            elif max_single_diff_pct < 0.1:   # 0.1%ä»¥å†…è®¤ä¸ºåŸºæœ¬ä¸€è‡´
                print(f"         ç»“æœ: âš ï¸  åŸºæœ¬ä¸€è‡´")
            else:
                print(f"         ç»“æœ: âŒ å­˜åœ¨å·®å¼‚")
        
        consistency_results[price_col] = {
            'max_diff': max_diff,
            'max_diff_pct': max_diff_pct
        }
    
    # æ€»ä½“ä¸€è‡´æ€§è¯„ä¼°
    print(f"\\nğŸ“Š æ€»ä½“ä¸€è‡´æ€§è¯„ä¼°:")
    
    all_consistent = True
    for col, result in consistency_results.items():
        if result['max_diff_pct'] > 0.1:
            all_consistent = False
            print(f"   {col}: âŒ å­˜åœ¨æ˜¾è‘—å·®å¼‚ ({result['max_diff_pct']:.4f}%)")
        elif result['max_diff_pct'] > 0.01:
            print(f"   {col}: âš ï¸  è½»å¾®å·®å¼‚ ({result['max_diff_pct']:.4f}%)")
        else:
            print(f"   {col}: âœ… é«˜åº¦ä¸€è‡´ ({result['max_diff_pct']:.4f}%)")
    
    if all_consistent:
        print(f"\\nğŸ‰ å¤æƒæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡ï¼")
    else:
        print(f"\\nâš ï¸  å¤æƒæ•°æ®å­˜åœ¨ä¸ä¸€è‡´ï¼Œå»ºè®®:")
        print(f"     1. å›ºå®šå¤æƒåŸºå‡†æ—¥æœŸ")
        print(f"     2. ç¼“å­˜å¤æƒå› å­")
        print(f"     3. ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®æº")
    
    return all_consistent

def main():
    parser = argparse.ArgumentParser(description='å¤æƒä¸€è‡´æ€§éªŒè¯å·¥å…·')
    parser.add_argument('--symbol', required=True, help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--dates', help='æ£€æŸ¥æ—¥æœŸåˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”')
    parser.add_argument('--check-consistency', action='store_true', help='æ‰§è¡Œä¸€è‡´æ€§æ£€æŸ¥')
    parser.add_argument('--period', default='2024-01-01,2024-03-31', help='æ£€æŸ¥çš„å†å²æœŸé—´ï¼Œæ ¼å¼ï¼šå¼€å§‹æ—¥æœŸ,ç»“æŸæ—¥æœŸ')
    
    args = parser.parse_args()
    
    print("ğŸ” å¤æƒä¸€è‡´æ€§éªŒè¯å·¥å…·")
    print("=" * 40)
    
    # è§£ææœŸé—´
    try:
        start_date, end_date = args.period.split(',')
        base_period = (start_date.strip(), end_date.strip())
    except:
        print("âŒ æœŸé—´æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºï¼šYYYY-MM-DD,YYYY-MM-DD")
        return 1
    
    # è§£ææ£€æŸ¥æ—¥æœŸ
    if args.dates:
        fetch_dates = [date.strip() for date in args.dates.split(',')]
    elif args.check_consistency:
        # é»˜è®¤ä½¿ç”¨å‡ ä¸ªæµ‹è¯•æ—¥æœŸ
        fetch_dates = ['2024-06-01', '2024-09-01', '2024-12-01']
    else:
        print("âŒ è¯·æŒ‡å®š --dates æˆ–ä½¿ç”¨ --check-consistency")
        return 1
    
    # è·å–æ•°æ®
    data_dict = fetch_data_at_different_times(args.symbol, base_period, fetch_dates)
    
    if not data_dict:
        print("âŒ æ²¡æœ‰è·å–åˆ°æœ‰æ•ˆæ•°æ®")
        return 1
    
    # æ‰§è¡Œä¸€è‡´æ€§æ£€æŸ¥
    consistent = compare_adjustment_consistency(data_dict)
    
    return 0 if consistent else 1

if __name__ == "__main__":
    sys.exit(main())
'''

# =============================================================================
# check_price_relationships.py - æ£€æŸ¥ä»·æ ¼å…³ç³»
# =============================================================================

CHECK_PRICE_RELATIONSHIPS_SCRIPT = '''#!/usr/bin/env python3
"""
ä»·æ ¼å…³ç³»æ£€æŸ¥å·¥å…·
æ£€æŸ¥è‚¡ç¥¨æ•°æ®ä¸­çš„ä»·æ ¼å…³ç³»æ˜¯å¦åˆç†

ä½¿ç”¨æ–¹å¼:
python scripts/check_price_relationships.py --input data/processed/
python scripts/check_price_relationships.py --file data/processed/000001.SZ.csv
"""

import sys
import pandas as pd
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

def check_single_file(file_path):
    """æ£€æŸ¥å•ä¸ªæ–‡ä»¶çš„ä»·æ ¼å…³ç³»"""
    try:
        df = pd.read_csv(file_path)
        
        if df.empty:
            return {
                'file': str(file_path),
                'valid': False,
                'error': 'æ–‡ä»¶ä¸ºç©º'
            }
        
        price_cols = ['open', 'high', 'low', 'close']
        missing_cols = [col for col in price_cols if col not in df.columns]
        
        if missing_cols:
            return {
                'file': str(file_path),
                'valid': False,
                'error': f'ç¼ºå°‘ä»·æ ¼åˆ—: {missing_cols}'
            }
        
        issues = []
        total_rows = len(df)
        
        # æ£€æŸ¥ high >= low
        high_low_violations = (df['high'] < df['low']).sum()
        if high_low_violations > 0:
            issues.append({
                'type': 'high_low_violation',
                'count': high_low_violations,
                'percentage': high_low_violations / total_rows * 100,
                'description': 'high < low'
            })
        
        # æ£€æŸ¥ high >= open
        high_open_violations = (df['high'] < df['open']).sum()
        if high_open_violations > 0:
            issues.append({
                'type': 'high_open_violation',
                'count': high_open_violations,
                'percentage': high_open_violations / total_rows * 100,
                'description': 'high < open'
            })
        
        # æ£€æŸ¥ high >= close
        high_close_violations = (df['high'] < df['close']).sum()
        if high_close_violations > 0:
            issues.append({
                'type': 'high_close_violation',
                'count': high_close_violations,
                'percentage': high_close_violations / total_rows * 100,
                'description': 'high < close'
            })
        
        # æ£€æŸ¥ low <= open
        low_open_violations = (df['low'] > df['open']).sum()
        if low_open_violations > 0:
            issues.append({
                'type': 'low_open_violation',
                'count': low_open_violations,
                'percentage': low_open_violations / total_rows * 100,
                'description': 'low > open'
            })
        
        # æ£€æŸ¥ low <= close
        low_close_violations = (df['low'] > df['close']).sum()
        if low_close_violations > 0:
            issues.append({
                'type': 'low_close_violation',
                'count': low_close_violations,
                'percentage': low_close_violations / total_rows * 100,
                'description': 'low > close'
            })
        
        # æ£€æŸ¥è´Ÿä»·æ ¼
        negative_prices = 0
        negative_price_details = []
        for col in price_cols:
            neg_count = (df[col] <= 0).sum()
            if neg_count > 0:
                negative_prices += neg_count
                negative_price_details.append(f'{col}: {neg_count}')
        
        if negative_prices > 0:
            issues.append({
                'type': 'negative_price',
                'count': negative_prices,
                'percentage': negative_prices / (total_rows * 4) * 100,  # 4ä¸ªä»·æ ¼åˆ—
                'description': f'è´Ÿä»·æ ¼æˆ–é›¶ä»·æ ¼: {", ".join(negative_price_details)}'
            })
        
        # æ£€æŸ¥å¼‚å¸¸æ³¢åŠ¨
        if len(df) > 1:
            for col in price_cols:
                pct_change = df[col].pct_change().abs()
                extreme_changes = (pct_change > 0.15).sum()  # å•æ—¥æ¶¨è·Œå¹…è¶…è¿‡15%
                
                if extreme_changes > total_rows * 0.1:  # è¶…è¿‡10%çš„äº¤æ˜“æ—¥å‡ºç°æç«¯æ³¢åŠ¨
                    issues.append({
                        'type': 'extreme_volatility',
                        'count': extreme_changes,
                        'percentage': extreme_changes / total_rows * 100,
                        'description': f'{col} æç«¯æ³¢åŠ¨è¿‡å¤š(>15%)'
                    })
        
        return {
            'file': str(file_path),
            'valid': len(issues) == 0,
            'total_rows': total_rows,
            'issues': issues
        }
        
    except Exception as e:
        return {
            'file': str(file_path),
            'valid': False,
            'error': str(e)
        }

def check_directory(dir_path):
    """æ£€æŸ¥ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    dir_path = Path(dir_path)
    csv_files = list(dir_path.glob("*.csv"))
    
    results = []
    
    print(f"ğŸ” æ£€æŸ¥ç›®å½•: {dir_path}")
    print(f"   æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    for csv_file in csv_files:
        print(f"   æ£€æŸ¥: {csv_file.name}")
        result = check_single_file(csv_file)
        results.append(result)
    
    return results

def print_check_results(results):
    """æ‰“å°æ£€æŸ¥ç»“æœ"""
    valid_count = sum(1 for r in results if r['valid'])
    total_count = len(results)
    
    print(f"\\nğŸ“Š ä»·æ ¼å…³ç³»æ£€æŸ¥ç»“æœ:")
    print("=" * 50)
    print(f"   æ€»æ–‡ä»¶æ•°: {total_count}")
    print(f"   æ­£å¸¸æ–‡ä»¶: {valid_count}")
    print(f"   å¼‚å¸¸æ–‡ä»¶: {total_count - valid_count}")
    print(f"   æ­£å¸¸ç‡: {valid_count/total_count*100:.1f}%")
    
    # ç»Ÿè®¡é—®é¢˜ç±»å‹
    issue_stats = {}
    total_issues = 0
    
    for result in results:
        if not result['valid'] and 'issues' in result:
            for issue in result['issues']:
                issue_type = issue['type']
                if issue_type not in issue_stats:
                    issue_stats[issue_type] = {
                        'count': 0,
                        'files': 0,
                        'total_violations': 0
                    }
                issue_stats[issue_type]['files'] += 1
                issue_stats[issue_type]['total_violations'] += issue['count']
                total_issues += issue['count']
    
    if issue_stats:
        print(f"\\nâš ï¸  é—®é¢˜ç±»å‹ç»Ÿè®¡:")
        for issue_type, stats in issue_stats.items():
            print(f"   {issue_type}:")
            print(f"     å½±å“æ–‡ä»¶: {stats['files']} ä¸ª")
            print(f"     è¿è§„è®°å½•: {stats['total_violations']} æ¡")
    
    # è¯¦ç»†ç»“æœ
    print(f"\\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    
    for result in results:
        file_name = Path(result['file']).name
        
        if result['valid']:
            print(f"   âœ… {file_name} - æ­£å¸¸ ({result.get('total_rows', 0)} è¡Œ)")
        else:
            print(f"   âŒ {file_name}")
            
            if 'error' in result:
                print(f"      é”™è¯¯: {result['error']}")
            elif 'issues' in result:
                for issue in result['issues']:
                    print(f"      é—®é¢˜: {issue['description']} - {issue['count']} æ¡ ({issue['percentage']:.1f}%)")

def main():
    parser = argparse.ArgumentParser(description='ä»·æ ¼å…³ç³»æ£€æŸ¥å·¥å…·')
    parser.add_argument('--input', help='è¾“å…¥ç›®å½•è·¯å¾„')
    parser.add_argument('--file', help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', help='è¾“å‡ºæ£€æŸ¥æŠ¥å‘Šåˆ°JSONæ–‡ä»¶')
    
    args = parser.parse_args()
    
    print("ğŸ’° ä»·æ ¼å…³ç³»æ£€æŸ¥å·¥å…·")
    print("=" * 40)
    
    if args.file:
        input_path = Path(args.file)
        if not input_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
            return 1
        results = [check_single_file(input_path)]
    elif args.input:
        input_path = Path(args.input)
        if not input_path.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {input_path}")
            return 1
        results = check_directory(input_path)
    else:
        print("âŒ è¯·æŒ‡å®š --input ç›®å½•æˆ– --file æ–‡ä»¶")
        return 1
    
    print_check_results(results)
    
    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        import json
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        print(f"\\nğŸ“„ æ£€æŸ¥æŠ¥å‘Šå·²ä¿å­˜: {args.output}")
    
    # è¿”å›çŠ¶æ€ç 
    valid_count = sum(1 for r in results if r['valid'])
    return 0 if valid_count == len(results) else 1

if __name__ == "__main__":
    sys.exit(main())
'''

# =============================================================================
# fix_price_relationships.py - ä¿®å¤ä»·æ ¼å…³ç³»å¼‚å¸¸
# =============================================================================

FIX_PRICE_RELATIONSHIPS_SCRIPT = '''#!/usr/bin/env python3
"""
ä»·æ ¼å…³ç³»ä¿®å¤å·¥å…·
è‡ªåŠ¨ä¿®å¤è‚¡ç¥¨æ•°æ®ä¸­çš„ä»·æ ¼å…³ç³»å¼‚å¸¸

ä½¿ç”¨æ–¹å¼:
python scripts/fix_price_relationships.py --input data/raw/000001.SZ.csv --output data/cleaned/
python scripts/fix_price_relationships.py --input data/raw/ --batch --output data/cleaned/
"""

import sys
import pandas as pd
import numpy as np
import argparse
from pathlib import Path
import shutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

def fix_price_relationships(df, method='interpolate'):
    """ä¿®å¤ä»·æ ¼å…³ç³»å¼‚å¸¸"""
    df = df.copy()
    fix_log = []
    
    price_cols = ['open', 'high', 'low', 'close']
    
    # æ£€æŸ¥å¿…éœ€åˆ—
    missing_cols = [col for col in price_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„ä»·æ ¼åˆ—: {missing_cols}")
    
    original_rows = len(df)
    
    # 1. ä¿®å¤è´Ÿä»·æ ¼å’Œé›¶ä»·æ ¼
    for col in price_cols:
        negative_mask = df[col] <= 0
        negative_count = negative_mask.sum()
        
        if negative_count > 0:
            if method == 'interpolate':
                # ä½¿ç”¨æ’å€¼æ–¹æ³•
                df.loc[negative_mask, col] = np.nan
                df[col] = df[col].interpolate(method='linear')
            elif method == 'forward_fill':
                # å‘å‰å¡«å……
                df.loc[negative_mask, col] = np.nan
                df[col] = df[col].fillna(method='ffill')
            elif method == 'drop':
                # åˆ é™¤å¼‚å¸¸è¡Œ
                df = df[~negative_mask]
            
            fix_log.append(f"ä¿®å¤ {col} åˆ—è´Ÿä»·æ ¼: {negative_count} ä¸ª")
    
    # 2. ä¿®å¤ high < low å¼‚å¸¸
    high_low_mask = df['high'] < df['low']
    high_low_count = high_low_mask.sum()
    
    if high_low_count > 0:
        # äº¤æ¢ high å’Œ low å€¼
        df.loc[high_low_mask, ['high', 'low']] = df.loc[high_low_mask, ['low', 'high']].values
        fix_log.append(f"ä¿®å¤ high < low å¼‚å¸¸: {high_low_count} ä¸ª")
    
    # 3. ä¿®å¤ high < open æˆ– high < close
    high_open_mask = df['high'] < df['open']
    high_open_count = high_open_mask.sum()
    if high_open_count > 0:
        df.loc[high_open_mask, 'high'] = df.loc[high_open_mask, 'open']
        fix_log.append(f"ä¿®å¤ high < open å¼‚å¸¸: {high_open_count} ä¸ª")
    
    high_close_mask = df['high'] < df['close']
    high_close_count = high_close_mask.sum()
    if high_close_count > 0:
        df.loc[high_close_mask, 'high'] = df.loc[high_close_mask, 'close']
        fix_log.append(f"ä¿®å¤ high < close å¼‚å¸¸: {high_close_count} ä¸ª")
    
    # 4. ä¿®å¤ low > open æˆ– low > close
    low_open_mask = df['low'] > df['open']
    low_open_count = low_open_mask.sum()
    if low_open_count > 0:
        df.loc[low_open_mask, 'low'] = df.loc[low_open_mask, 'open']
        fix_log.append(f"ä¿®å¤ low > open å¼‚å¸¸: {low_open_count} ä¸ª")
    
    low_close_mask = df['low'] > df['close']
    low_close_count = low_close_mask.sum()
    if low_close_count > 0:
        df.loc[low_close_mask, 'low'] = df.loc[low_close_mask, 'close']
        fix_log.append(f"ä¿®å¤ low > close å¼‚å¸¸: {low_close_count} ä¸ª")
    
    # 5. å¤„ç†å¼‚å¸¸æ³¢åŠ¨ï¼ˆå¯é€‰ï¼‰
    if method == 'smooth_volatility':
        for col in price_cols:
            if len(df) > 1:
                pct_change = df[col].pct_change().abs()
                extreme_mask = pct_change > 0.2  # è¶…è¿‡20%çš„å˜åŠ¨
                extreme_count = extreme_mask.sum()
                
                if extreme_count > 0:
                    # ä½¿ç”¨æ»‘åŠ¨å¹³å‡å¹³æ»‘å¼‚å¸¸æ³¢åŠ¨
                    smoothed = df[col].rolling(window=3, center=True).mean()
                    df.loc[extreme_mask, col] = smoothed.loc[extreme_mask]
                    fix_log.append(f"å¹³æ»‘ {col} åˆ—å¼‚å¸¸æ³¢åŠ¨: {extreme_count} ä¸ª")
    
    # 6. åˆ é™¤ä»ç„¶æ— æ•ˆçš„è¡Œ
    if method != 'keep_invalid':
        # æ£€æŸ¥ä¿®å¤åä»æœ‰é—®é¢˜çš„è¡Œ
        invalid_mask = (
            (df['high'] < df['low']) |
            (df['high'] < df['open']) |
            (df['high'] < df['close']) |
            (df['low'] > df['open']) |
            (df['low'] > df['close']) |
            (df[price_cols] <= 0).any(axis=1)
        )
        
        invalid_count = invalid_mask.sum()
        if invalid_count > 0:
            df = df[~invalid_mask]
            fix_log.append(f"åˆ é™¤æ— æ³•ä¿®å¤çš„è¡Œ: {invalid_count} ä¸ª")
    
    final_rows = len(df)
    if final_rows != original_rows:
        fix_log.append(f"æ•°æ®è¡Œæ•°å˜åŒ–: {original_rows} -> {final_rows}")
    
    return df, fix_log

def fix_single_file(input_file, output_file, method='interpolate', backup=True):
    """ä¿®å¤å•ä¸ªæ–‡ä»¶"""
    try:
        # å¤‡ä»½åŸæ–‡ä»¶
        if backup:
            backup_file = input_file.with_suffix(input_file.suffix + '.backup')
            shutil.copy2(input_file, backup_file)
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(input_file)
        
        if df.empty:
            return False, "æ–‡ä»¶ä¸ºç©º"
        
        # ä¿®å¤ä»·æ ¼å…³ç³»
        fixed_df, fix_log = fix_price_relationships(df, method)
        
        # ä¿å­˜ä¿®å¤åçš„æ•°æ®
        output_file.parent.mkdir(parents=True, exist_ok=True)
        fixed_df.to_csv(output_file, index=False)
        
        return True, fix_log
        
    except Exception as e:
        return False, str(e)

def batch_fix_files(input_dir, output_dir, method='interpolate', backup=True):
    """æ‰¹é‡ä¿®å¤æ–‡ä»¶"""
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    
    csv_files = list(input_dir.glob("*.csv"))
    
    if not csv_files:
        print("æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    success_count = 0
    total_fixes = 0
    
    for csv_file in csv_files:
        output_file = output_dir / csv_file.name
        
        print(f"\\nä¿®å¤: {csv_file.name}")
        
        success, result = fix_single_file(csv_file, output_file, method, backup)
        
        if success:
            fix_log = result
            success_count += 1
            fixes_count = len(fix_log)
            total_fixes += fixes_count
            
            print(f"   âœ… ä¿®å¤æˆåŠŸ: {fixes_count} é¡¹ä¿®å¤")
            for log_entry in fix_log:
                print(f"      â€¢ {log_entry}")
        else:
            print(f"   âŒ ä¿®å¤å¤±è´¥: {result}")
    
    print(f"\\nğŸ“Š æ‰¹é‡ä¿®å¤å®Œæˆ:")
    print(f"   æˆåŠŸæ–‡ä»¶: {success_count}/{len(csv_files)}")
    print(f"   æ€»ä¿®å¤é¡¹: {total_fixes}")

def main():
    parser = argparse.ArgumentParser(description='ä»·æ ¼å…³ç³»ä¿®å¤å·¥å…·')
    parser.add_argument('--input', required=True, help='è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•')
    parser.add_argument('--output', required=True, help='è¾“å‡ºæ–‡ä»¶æˆ–ç›®å½•')
    parser.add_argument('--method', 
                       choices=['interpolate', 'forward_fill', 'drop', 'smooth_volatility', 'keep_invalid'],
                       default='interpolate',
                       help='ä¿®å¤æ–¹æ³•')
    parser.add_argument('--batch', action='store_true', help='æ‰¹é‡å¤„ç†æ¨¡å¼')
    parser.add_argument('--no-backup', action='store_true', help='ä¸åˆ›å»ºå¤‡ä»½æ–‡ä»¶')
    
    args = parser.parse_args()
    
    print("ğŸ”§ ä»·æ ¼å…³ç³»ä¿®å¤å·¥å…·")
    print("=" * 40)
    print(f"ä¿®å¤æ–¹æ³•: {args.method}")
    
    input_path = Path(args.input)
    output_path = Path(args.output)
    
    if not input_path.exists():
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return 1
    
    backup = not args.no_backup
    
    if args.batch or input_path.is_dir():
        if not input_path.is_dir():
            print(f"âŒ æ‰¹é‡æ¨¡å¼è¦æ±‚è¾“å…¥ä¸ºç›®å½•")
            return 1
        
        batch_fix_files(input_path, output_path, args.method, backup)
    else:
        if input_path.is_file():
            success, result = fix_single_file(input_path, output_path, args.method, backup)
            
            if success:
                fix_log = result
                print(f"âœ… æ–‡ä»¶ä¿®å¤æˆåŠŸ: {len(fix_log)} é¡¹ä¿®å¤")
                for log_entry in fix_log:
                    print(f"   â€¢ {log_entry}")
                return 0
            else:
                print(f"âŒ æ–‡ä»¶ä¿®å¤å¤±è´¥: {result}")
                return 1
        else:
            print(f"âŒ å•æ–‡ä»¶æ¨¡å¼è¦æ±‚è¾“å…¥ä¸ºæ–‡ä»¶")
            return 1

if __name__ == "__main__":
    sys.exit(main())
'''

# =============================================================================
# align_trading_calendar.py - å¯¹é½äº¤æ˜“æ—¥å†
# =============================================================================

ALIGN_TRADING_CALENDAR_SCRIPT = '''#!/usr/bin/env python3
"""
äº¤æ˜“æ—¥å†å¯¹é½å·¥å…·
æ ‡å‡†åŒ–ä¸åŒæ•°æ®æºçš„äº¤æ˜“æ—¥å†ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§

ä½¿ç”¨æ–¹å¼:
python scripts/align_trading_calendar.py --input data/raw/ --output data/aligned/
python scripts/align_trading_calendar.py --file data/raw/000001.SZ.csv --calendar SHSZ
"""

import sys
import pandas as pd
import argparse
from pathlib import Path
from datetime import datetime, date

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

def get_trading_calendar(calendar_name='SHSZ', start_date=None, end_date=None):
    """è·å–äº¤æ˜“æ—¥å†"""
    try:
        from zipline.utils.calendars import get_calendar
        
        calendar = get_calendar(calendar_name)
        
        if start_date and end_date:
            sessions = calendar.sessions_in_range(start_date, end_date)
            return [session.date() for session in sessions]
        else:
            # è¿”å›å½“å¹´çš„äº¤æ˜“æ—¥
            current_year = datetime.now().year
            start = f"{current_year}-01-01"
            end = f"{current_year}-12-31"
            sessions = calendar.sessions_in_range(start, end)
            return [session.date() for session in sessions]
    
    except ImportError:
        print("âš ï¸  Ziplineæœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–äº¤æ˜“æ—¥å†")
        return get_simple_trading_calendar(start_date, end_date)

def get_simple_trading_calendar(start_date=None, end_date=None):
    """ç®€åŒ–çš„äº¤æ˜“æ—¥å†ï¼ˆæ’é™¤å‘¨æœ«å’Œä¸»è¦èŠ‚å‡æ—¥ï¼‰"""
    if not start_date:
        start_date = "2024-01-01"
    if not end_date:
        end_date = "2024-12-31"
    
    # ä¸»è¦èŠ‚å‡æ—¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
    holidays = [
        "2024-01-01",  # å…ƒæ—¦
        "2024-02-10", "2024-02-11", "2024-02-12", "2024-02-13", "2024-02-14", "2024-02-15", "2024-02-16", "2024-02-17",  # æ˜¥èŠ‚
        "2024-04-04", "2024-04-05", "2024-04-06",  # æ¸…æ˜èŠ‚
        "2024-05-01", "2024-05-02", "2024-05-03",  # åŠ³åŠ¨èŠ‚
        "2024-06-10",  # ç«¯åˆèŠ‚
        "2024-09-15", "2024-09-16", "2024-09-17",  # ä¸­ç§‹èŠ‚
        "2024-10-01", "2024-10-02", "2024-10-03", "2024-10-04", "2024-10-05", "2024-10-06", "2024-10-07"  # å›½åº†èŠ‚
    ]
    
    holiday_dates = {datetime.strptime(h, "%Y-%m-%d").date() for h in holidays}
    
    # ç”Ÿæˆæ—¥æœŸèŒƒå›´
    date_range = pd.date_range(start_date, end_date, freq='D')
    
    # è¿‡æ»¤å‘¨æœ«å’ŒèŠ‚å‡æ—¥
    trading_days = []
    for dt in date_range:
        if dt.weekday() < 5 and dt.date() not in holiday_dates:  # å‘¨ä¸€åˆ°å‘¨äº”ï¼Œä¸”ä¸æ˜¯èŠ‚å‡æ—¥
            trading_days.append(dt.date())
    
    return trading_days

def align_data_to_calendar(df, trading_calendar, date_col='date'):
    """å°†æ•°æ®å¯¹é½åˆ°äº¤æ˜“æ—¥å†"""
    if date_col not in df.columns:
        raise ValueError(f"æ—¥æœŸåˆ— '{date_col}' ä¸å­˜åœ¨")
    
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col])
    df['date_only'] = df[date_col].dt.date
    
    # è·å–æ•°æ®ä¸­çš„æ—¥æœŸèŒƒå›´
    data_dates = set(df['date_only'])
    calendar_dates = set(trading_calendar)
    
    # ç»Ÿè®¡ä¿¡æ¯
    extra_dates = data_dates - calendar_dates  # æ•°æ®ä¸­æœ‰ä½†æ—¥å†ä¸­æ²¡æœ‰çš„æ—¥æœŸ
    missing_dates = calendar_dates - data_dates  # æ—¥å†ä¸­æœ‰ä½†æ•°æ®ä¸­æ²¡æœ‰çš„æ—¥æœŸ
    
    # è¿‡æ»¤æ‰éäº¤æ˜“æ—¥çš„æ•°æ®
    aligned_df = df[df['date_only'].isin(calendar_dates)].copy()
    aligned_df = aligned_df.drop('date_only', axis=1)
    aligned_df = aligned_df.sort_values(date_col).reset_index(drop=True)
    
    align_info = {
        'original_rows': len(df),
        'aligned_rows': len(aligned_df),
        'extra_dates': len(extra_dates),
        'missing_dates': len(missing_dates),
        'removed_dates': list(extra_dates)[:10] if extra_dates else [],  # åªæ˜¾ç¤ºå‰10ä¸ª
        'missing_dates_list': list(missing_dates)[:10] if missing_dates else []
    }
    
    return aligned_df, align_info

def align_single_file(input_file, output_file, calendar_name='SHSZ'):
    """å¯¹é½å•ä¸ªæ–‡ä»¶åˆ°äº¤æ˜“æ—¥å†"""
    try:
        # è¯»å–æ•°æ®
        df = pd.read_csv(input_file)
        
        if df.empty:
            return False, "æ–‡ä»¶ä¸ºç©º"
        
        # æ£€æŸ¥æ—¥æœŸåˆ—
        date_cols = [col for col in df.columns if 'date' in col.lower()]
        if not date_cols:
            return False, "æœªæ‰¾åˆ°æ—¥æœŸåˆ—"
        
        date_col = date_cols[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ—¥æœŸåˆ—
        
        # è·å–æ•°æ®æ—¥æœŸèŒƒå›´
        df_temp = df.copy()
        df_temp[date_col] = pd.to_datetime(df_temp[date_col])
        start_date = df_temp[date_col].min().strftime('%Y-%m-%d')
        end_date = df_temp[date_col].max().strftime('%Y-%m-%d')
        
        # è·å–äº¤æ˜“æ—¥å†
        trading_calendar = get_trading_calendar(calendar_name, start_date, end_date)
        
        # å¯¹é½æ•°æ®
        aligned_df, align_info = align_data_to_calendar(df, trading_calendar, date_col)
        
        # ä¿å­˜ç»“æœ
        output_file.parent.mkdir(parents=True, exist_ok=True)
        aligned_df.to_csv(output_file, index=False)
        
        return True, align_info
        
    except Exception as e:
        return False, str(e)

def batch_align_files(input_dir, output_dir, calendar_name='SHSZ'):
    """æ‰¹é‡å¯¹é½æ–‡ä»¶"""
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    
    csv_files = list(input_dir.glob("*.csv"))
    
    if not csv_files:
        print("æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    success_count = 0
    total_removed = 0
    total_original = 0
    
    for csv_file in csv_files:
        output_file = output_dir / csv_file.name
        
        print(f"\\nå¯¹é½: {csv_file.name}")
        
        success, result = align_single_file(csv_file, output_file, calendar_name)
        
        if success:
            align_info = result
            success_count += 1
            total_removed += align_info['extra_dates']
            total_original += align_info['original_rows']
            
            print(f"   âœ… å¯¹é½æˆåŠŸ")
            print(f"      åŸå§‹è¡Œæ•°: {align_info['original_rows']}")
            print(f"      å¯¹é½åè¡Œæ•°: {align_info['aligned_rows']}")
            print(f"      ç§»é™¤éäº¤æ˜“æ—¥: {align_info['extra_dates']} å¤©")
            print(f"      ç¼ºå¤±äº¤æ˜“æ—¥: {align_info['missing_dates']} å¤©")
            
            if align_info['removed_dates']:
                print(f"      ç§»é™¤çš„æ—¥æœŸç¤ºä¾‹: {', '.join(map(str, align_info['removed_dates'][:5]))}")
        else:
            print(f"   âŒ å¯¹é½å¤±è´¥: {result}")
    
    print(f"\\nğŸ“Š æ‰¹é‡å¯¹é½å®Œæˆ:")
    print(f"   æˆåŠŸæ–‡ä»¶: {success_count}/{len(csv_files)}")
    print(f"   æ€»ç§»é™¤è¡Œæ•°: {total_removed}")
    if total_original > 0:
        print(f"   æ•°æ®ä¿ç•™ç‡: {((total_original - total_removed) / total_original * 100):.1f}%")

def main():
    parser = argparse.ArgumentParser(description='äº¤æ˜“æ—¥å†å¯¹é½å·¥å…·')
    parser.add_argument('--input', help='è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶æˆ–ç›®å½•')
    parser.add_argument('--file', help='å•æ–‡ä»¶å¤„ç†æ¨¡å¼')
    parser.add_argument('--calendar', default='SHSZ', 
                       choices=['SHSZ', 'XSHG', 'XSHE'],
                       help='äº¤æ˜“æ—¥å†ç±»å‹')
    parser.add_argument('--batch', action='store_true', help='æ‰¹é‡å¤„ç†æ¨¡å¼')
    
    args = parser.parse_args()
    
    print("ğŸ“… äº¤æ˜“æ—¥å†å¯¹é½å·¥å…·")
    print("=" * 40)
    print(f"ä½¿ç”¨æ—¥å†: {args.calendar}")
    
    if args.file:
        # å•æ–‡ä»¶æ¨¡å¼
        input_file = Path(args.file)
        if not input_file.exists():
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            return 1
        
        output_file = Path(args.output) if args.output else input_file.with_suffix('.aligned.csv')
        
        success, result = align_single_file(input_file, output_file, args.calendar)
        
        if success:
            align_info = result
            print(f"âœ… æ–‡ä»¶å¯¹é½æˆåŠŸ:")
            print(f"   åŸå§‹è¡Œæ•°: {align_info['original_rows']}")
            print(f"   å¯¹é½åè¡Œæ•°: {align_info['aligned_rows']}")
            print(f"   ç§»é™¤éäº¤æ˜“æ—¥: {align_info['extra_dates']} å¤©")
            print(f"   ç¼ºå¤±äº¤æ˜“æ—¥: {align_info['missing_dates']} å¤©")
            return 0
        else:
            print(f"âŒ æ–‡ä»¶å¯¹é½å¤±è´¥: {result}")
            return 1
    
    elif args.input and args.output:
        # æ‰¹é‡æ¨¡å¼
        input_path = Path(args.input)
        output_path = Path(args.output)
        
        if not input_path.exists():
            print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
            return 1
        
        if input_path.is_dir():
            batch_align_files(input_path, output_path, args.calendar)
        else:
            success, result = align_single_file(input_path, output_path, args.calendar)
            if success:
                align_info = result
                print(f"âœ… å•æ–‡ä»¶å¯¹é½æˆåŠŸ: ç§»é™¤ {align_info['extra_dates']} ä¸ªéäº¤æ˜“æ—¥")
                return 0
            else:
                print(f"âŒ å•æ–‡ä»¶å¯¹é½å¤±è´¥: {result}")
                return 1
    else:
        print("âŒ è¯·æŒ‡å®šè¾“å…¥è¾“å‡ºè·¯å¾„ï¼Œæˆ–ä½¿ç”¨ --file æ¨¡å¼")
        return 1

if __name__ == "__main__":
    sys.exit(main())
'''

# =============================================================================
# validate_zipline_format.py - éªŒè¯Ziplineæ ¼å¼
# =============================================================================

VALIDATE_ZIPLINE_FORMAT_SCRIPT = '''#!/usr/bin/env python3
"""
Ziplineæ ¼å¼éªŒè¯å·¥å…·
éªŒè¯CSVæ•°æ®æ˜¯å¦ç¬¦åˆZiplineè¦æ±‚çš„æ ¼å¼

ä½¿ç”¨æ–¹å¼:
python scripts/validate_zipline_format.py --input data/zipline/
python scripts/validate_zipline_format.py --file data/zipline/000001.SZ.csv
"""

import sys
import pandas as pd
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Ziplineæ‰€éœ€çš„åˆ—åå’Œç±»å‹
ZIPLINE_REQUIRED_COLUMNS = {
    'date': 'datetime',
    'open': 'float',
    'high': 'float',
    'low': 'float',
    'close': 'float',
    'volume': 'int'
}

ZIPLINE_OPTIONAL_COLUMNS = {
    'dividend': 'float',
    'split': 'float'
}

def validate_zipline_format(df, filename):
    """éªŒè¯å•ä¸ªDataFrameçš„Ziplineæ ¼å¼"""
    issues = []
    warnings = []
    
    # 1. æ£€æŸ¥å¿…éœ€åˆ—
    missing_columns = []
    for col in ZIPLINE_REQUIRED_COLUMNS.keys():
        if col not in df.columns:
            missing_columns.append(col)
    
    if missing_columns:
        issues.append(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")
        return {
            'valid': False,
            'issues': issues,
            'warnings': warnings
        }
    
    # 2. æ£€æŸ¥æ•°æ®ç±»å‹
    for col, expected_type in ZIPLINE_REQUIRED_COLUMNS.items():
        if col in df.columns:
            if expected_type == 'datetime':
                try:
                    pd.to_datetime(df[col])
                except:
                    issues.append(f"åˆ— {col} ä¸æ˜¯æœ‰æ•ˆçš„æ—¥æœŸæ ¼å¼")
            elif expected_type == 'float':
                if not pd.api.types.is_numeric_dtype(df[col]):
                    issues.append(f"åˆ— {col} åº”ä¸ºæ•°å€¼ç±»å‹")
                elif df[col].dtype != 'float64':
                    warnings.append(f"åˆ— {col} ç±»å‹ä¸º {df[col].dtype}ï¼Œå»ºè®®è½¬æ¢ä¸º float64")
            elif expected_type == 'int':
                if not pd.api.types.is_numeric_dtype(df[col]):
                    issues.append(f"åˆ— {col} åº”ä¸ºæ•°å€¼ç±»å‹")
                elif not df[col].dtype.name.startswith('int'):
                    warnings.append(f"åˆ— {col} ç±»å‹ä¸º {df[col].dtype}ï¼Œå»ºè®®è½¬æ¢ä¸º int64")
    
    # 3. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    if df.empty:
        issues.append("æ•°æ®ä¸ºç©º")
    else:
        # æ£€æŸ¥ç¼ºå¤±å€¼
        null_counts = df[list(ZIPLINE_REQUIRED_COLUMNS.keys())].isnull().sum()
        null_columns = null_counts[null_counts > 0]
        
        if len(null_columns) > 0:
            issues.append(f"å­˜åœ¨ç¼ºå¤±å€¼: {dict(null_columns)}")
        
        # æ£€æŸ¥è´Ÿä»·æ ¼
        price_cols = ['open', 'high', 'low', 'close']
        for col in price_cols:
            if col in df.columns:
                negative_count = (df[col] <= 0).sum()
                if negative_count > 0:
                    issues.append(f"åˆ— {col} å­˜åœ¨ {negative_count} ä¸ªéæ­£å€¼")
        
        # æ£€æŸ¥è´Ÿæˆäº¤é‡
        if 'volume' in df.columns:
            negative_volume = (df['volume'] < 0).sum()
            if negative_volume > 0:
                issues.append(f"æˆäº¤é‡å­˜åœ¨ {negative_volume} ä¸ªè´Ÿå€¼")
        
        # æ£€æŸ¥ä»·æ ¼å…³ç³»
        if all(col in df.columns for col in price_cols):
            high_low_issues = (df['high'] < df['low']).sum()
            if high_low_issues > 0:
                issues.append(f"{high_low_issues} è¡Œæ•°æ® high < low")
            
            high_open_issues = (df['high'] < df['open']).sum()
            if high_open_issues > 0:
                issues.append(f"{high_open_issues} è¡Œæ•°æ® high < open")
            
            high_close_issues = (df['high'] < df['close']).sum()
            if high_close_issues > 0:
                issues.append(f"{high_close_issues} è¡Œæ•°æ® high < close")
            
            low_open_issues = (df['low'] > df['open']).sum()
            if low_open_issues > 0:
                issues.append(f"{low_open_issues} è¡Œæ•°æ® low > open")
            
            low_close_issues = (df['low'] > df['close']).sum()
            if low_close_issues > 0:
                issues.append(f"{low_close_issues} è¡Œæ•°æ® low > close")
    
    # 4. æ£€æŸ¥æ—¥æœŸæ ¼å¼å’Œæ’åº
    if 'date' in df.columns:
        try:
            df_temp = df.copy()
            df_temp['date'] = pd.to_datetime(df_temp['date'])
            
            # æ£€æŸ¥æ—¥æœŸæ’åº
            if not df_temp['date'].is_monotonic_increasing:
                warnings.append("æ—¥æœŸæœªæŒ‰å‡åºæ’åˆ—")
            
            # æ£€æŸ¥é‡å¤æ—¥æœŸ
            duplicates = df_temp['date'].duplicated().sum()
            if duplicates > 0:
                issues.append(f"å­˜åœ¨ {duplicates} ä¸ªé‡å¤æ—¥æœŸ")
            
            # æ£€æŸ¥æ—¥æœŸèŒƒå›´åˆç†æ€§
            min_date = df_temp['date'].min()
            max_date = df_temp['date'].max()
            
            if min_date.year < 1990:
                warnings.append(f"æœ€æ—©æ—¥æœŸè¿‡æ—©: {min_date.date()}")
            
            if max_date > pd.Timestamp.now():
                warnings.append(f"æœ€æ™šæ—¥æœŸè¶…è¿‡å½“å‰æ—¥æœŸ: {max_date.date()}")
            
        except Exception as e:
            issues.append(f"æ—¥æœŸå¤„ç†é”™è¯¯: {e}")
    
    # 5. æ£€æŸ¥æ–‡ä»¶åæ ¼å¼
    if filename:
        if not filename.endswith('.csv'):
            warnings.append("æ–‡ä»¶ååº”ä»¥.csvç»“å°¾")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è‚¡ç¥¨ä»£ç 
        stem = Path(filename).stem
        if not any(char.isdigit() for char in stem):
            warnings.append("æ–‡ä»¶åä¸­æœªæ£€æµ‹åˆ°è‚¡ç¥¨ä»£ç ")
    
    return {
        'valid': len(issues) == 0,
        'issues': issues,
        'warnings': warnings,
        'rows': len(df),
        'date_range': (df['date'].min(), df['date'].max()) if 'date' in df.columns and not df.empty else None
    }

def validate_single_file(file_path):
    """éªŒè¯å•ä¸ªæ–‡ä»¶"""
    try:
        df = pd.read_csv(file_path)
        result = validate_zipline_format(df, file_path.name)
        result['file'] = str(file_path)
        return result
        
    except Exception as e:
        return {
            'file': str(file_path),
            'valid': False,
            'issues': [f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}"],
            'warnings': []
        }

def validate_directory(dir_path):
    """éªŒè¯ç›®å½•ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶"""
    dir_path = Path(dir_path)
    csv_files = list(dir_path.glob("*.csv"))
    
    if not csv_files:
        print("æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
        return []
    
    print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    results = []
    for csv_file in csv_files:
        print(f"éªŒè¯: {csv_file.name}")
        result = validate_single_file(csv_file)
        results.append(result)
    
    return results

def print_validation_summary(results):
    """æ‰“å°éªŒè¯æ‘˜è¦"""
    if not results:
        return
    
    valid_count = sum(1 for r in results if r['valid'])
    total_count = len(results)
    
    print(f"\\nğŸ“Š Ziplineæ ¼å¼éªŒè¯ç»“æœ:")
    print("=" * 50)
    print(f"æ€»æ–‡ä»¶æ•°: {total_count}")
    print(f"é€šè¿‡éªŒè¯: {valid_count}")
    print(f"éªŒè¯å¤±è´¥: {total_count - valid_count}")
    print(f"é€šè¿‡ç‡: {valid_count/total_count*100:.1f}%")
    
    # ç»Ÿè®¡å¸¸è§é—®é¢˜
    all_issues = []
    all_warnings = []
    
    for result in results:
        all_issues.extend(result.get('issues', []))
        all_warnings.extend(result.get('warnings', []))
    
    if all_issues:
        print(f"\\nâŒ å¸¸è§é—®é¢˜:")
        issue_counts = {}
        for issue in all_issues:
            issue_type = issue.split(':')[0] if ':' in issue else issue.split('å­˜åœ¨')[0] if 'å­˜åœ¨' in issue else issue
            issue_counts[issue_type] = issue_counts.get(issue_type, 0) + 1
        
        for issue_type, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"   â€¢ {issue_type}: {count} æ¬¡")
    
    if all_warnings:
        print(f"\\nâš ï¸  å¸¸è§è­¦å‘Š:")
        warning_counts = {}
        for warning in all_warnings:
            warning_type = warning.split(':')[0] if ':' in warning else warning.split('å»ºè®®')[0] if 'å»ºè®®' in warning else warning
            warning_counts[warning_type] = warning_counts.get(warning_type, 0) + 1
        
        for warning_type, count in sorted(warning_counts.items(), key=lambda x: x[1], reverse=True)[:3]:
            print(f"   â€¢ {warning_type}: {count} æ¬¡")

def print_detailed_results(results, show_warnings=False):
    """æ‰“å°è¯¦ç»†ç»“æœ"""
    print(f"\\nğŸ“‹ è¯¦ç»†éªŒè¯ç»“æœ:")
    
    for result in results:
        file_name = Path(result['file']).name
        
        if result['valid']:
            status_icon = "âœ…"
            status_text = "é€šè¿‡"
        else:
            status_icon = "âŒ"
            status_text = "å¤±è´¥"
        
        print(f"\\n{status_icon} {file_name} - {status_text}")
        
        if 'rows' in result:
            print(f"   è¡Œæ•°: {result['rows']}")
        
        if 'date_range' in result and result['date_range']:
            start_date, end_date = result['date_range']
            print(f"   æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
        
        # æ˜¾ç¤ºé—®é¢˜
        if result.get('issues'):
            for issue in result['issues']:
                print(f"   âŒ {issue}")
        
        # æ˜¾ç¤ºè­¦å‘Šï¼ˆå¯é€‰ï¼‰
        if show_warnings and result.get('warnings'):
            for warning in result['warnings']:
                print(f"   âš ï¸  {warning}")

def main():
    parser = argparse.ArgumentParser(description='Ziplineæ ¼å¼éªŒè¯å·¥å…·')
    parser.add_argument('--input', help='è¾“å…¥ç›®å½•è·¯å¾„')
    parser.add_argument('--file', help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--show-warnings', action='store_true', help='æ˜¾ç¤ºè­¦å‘Šä¿¡æ¯')
    parser.add_argument('--output', help='è¾“å‡ºéªŒè¯æŠ¥å‘Šåˆ°JSONæ–‡ä»¶')
    
    args = parser.parse_args()
    
    print("ğŸ“Š Ziplineæ ¼å¼éªŒè¯å·¥å…·")
    print("=" * 40)
    
    if args.file:
        # å•æ–‡ä»¶æ¨¡å¼
        file_path = Path(args.file)
        if not file_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return 1
        
        result = validate_single_file(file_path)
        results = [result]
    
    elif args.input:
        # ç›®å½•æ¨¡å¼
        input_path = Path(args.input)
        if not input_path.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {input_path}")
            return 1
        
        results = validate_directory(input_path)
    
    else:
        print("âŒ è¯·æŒ‡å®š --input ç›®å½•æˆ– --file æ–‡ä»¶")
        return 1
    
    # æ‰“å°ç»“æœ
    print_validation_summary(results)
    print_detailed_results(results, args.show_warnings)
    
    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        import json
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        print(f"\\nğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {args.output}")
    
    # è¿”å›çŠ¶æ€ç 
    valid_count = sum(1 for r in results if r['valid'])
    return 0 if valid_count == len(results) else 1

if __name__ == "__main__":
    sys.exit(main())
'''

    
    os.environ['ZIPLINE_ROOT'] = str(zipline_root)
    
    print(f"è®¾ç½® ZIPLINE_ROOT: {zipline_root}")
    return zipline_root

def create_bundle_definition(bundle_name, data_dir, symbols=None):
    """åˆ›å»ºBundleå®šä¹‰æ–‡ä»¶"""
    bundle_file = PROJECT_ROOT / 'zipline_extensions.py'
    
    # Bundleå®šä¹‰æ¨¡æ¿
    bundle_template = f'''
import pandas as pd
from zipline.data.bundles import register
from zipline.data.bundles.csvdir import csvdir_equities

# æ³¨å†Œè‡ªå®šä¹‰bundle
register(
    '{bundle_name}',
    csvdir_equities(
        ['{data_dir}'],
        symbol_columns=['symbol'],
        date_columns=['date'],
        timezone='Asia/Shanghai'
    ),
    calendar_name='XSHG',
    start_session=pd.Timestamp('2020-01-01', tz='utc'),
    end_session=pd.Timestamp('2024-12-31', tz='utc'),
)
'''
    
    with open(bundle_file, 'w', encoding='utf-8') as f:
        f.write(bundle_template.strip())
    
    print(f"åˆ›å»ºBundleå®šä¹‰æ–‡ä»¶: {bundle_file}")
    return bundle_file

def run_zipline_ingest(bundle_name, force=False):
    """è¿è¡ŒZiplineæ•°æ®æ‘„å…¥"""
    try:
        # æ„å»ºå‘½ä»¤
        cmd = ['zipline', 'ingest', '-b', bundle_name]
        
        if force:
            # æ¸…ç†ç°æœ‰æ•°æ®
            clean_cmd = ['zipline', 'clean', '-b', bundle_name, '--keep-last', '0']
            print(f"æ¸…ç†ç°æœ‰Bundleæ•°æ®...")
            clean_result = subprocess.run(clean_cmd, capture_output=True, text=True)
            if clean_result.returncode != 0:
                print(f"è­¦å‘Š: æ¸…ç†å¤±è´¥ - {clean_result.stderr}")
        
        print(f"å¼€å§‹æ‘„å…¥Bundle: {bundle_name}")
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        # æ‰§è¡Œæ‘„å…¥
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            print("æ‘„å…¥æˆåŠŸ!")
            if result.stdout:
                print(f"è¾“å‡º: {result.stdout}")
            return True
        else:
            print(f"æ‘„å…¥å¤±è´¥!")
            print(f"é”™è¯¯: {result.stderr}")
            return False
            
    except FileNotFoundError:
        print("é”™è¯¯: ziplineå‘½ä»¤æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿å·²å®‰è£…zipline-reloaded")
        return False
    except Exception as e:
        print(f"æ‘„å…¥è¿‡ç¨‹å¼‚å¸¸: {e}")
        return False

def verify_bundle(bundle_name):
    """éªŒè¯Bundleæ˜¯å¦å¯ç”¨"""
    try:
        cmd = ['zipline', 'bundles']
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            bundles_list = result.stdout
            if bundle_name in bundles_list:
                print(f"BundleéªŒè¯æˆåŠŸ: {bundle_name} å·²æ³¨å†Œ")
                return True
            else:
                print(f"BundleéªŒè¯å¤±è´¥: {bundle_name} æœªåœ¨åˆ—è¡¨ä¸­")
                print(f"å¯ç”¨çš„Bundle: {bundles_list}")
                return False
        else:
            print(f"æ— æ³•è·å–Bundleåˆ—è¡¨: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"BundleéªŒè¯å¼‚å¸¸: {e}")
        return False

def prepare_data_for_ingest(symbols=None):
    """å‡†å¤‡æ‘„å…¥æ•°æ®"""
    from backend.zipline_csv_writer import write_zipline_csv
    
    data_dir = PROJECT_ROOT / 'data' / 'zipline'
    
    if symbols is None:
        # é»˜è®¤è‚¡ç¥¨åˆ—è¡¨
        symbols = ['000001.SZ', '000002.SZ', '600000.SH', '600036.SH']
    
    print(f"å‡†å¤‡æ‘„å…¥æ•°æ®ï¼Œè‚¡ç¥¨æ•°é‡: {len(symbols)}")
    
    try:
        # ç”ŸæˆZiplineæ ¼å¼CSV
        result = write_zipline_csv(
            symbols=symbols,
            output_dir=str(data_dir),
            start_date='2024-01-01',
            end_date='2024-12-31',
            overwrite=True
        )
        
        if result.get('files_generated', 0) > 0:
            print(f"æ•°æ®å‡†å¤‡æˆåŠŸ: {result['files_generated']} ä¸ªæ–‡ä»¶")
            return True, data_dir
        else:
            print("æ•°æ®å‡†å¤‡å¤±è´¥: æ²¡æœ‰ç”Ÿæˆæ–‡ä»¶")
            return False, None
            
    except Exception as e:
        print(f"æ•°æ®å‡†å¤‡å¼‚å¸¸: {e}")
        return False, None

def main():
    parser = argparse.ArgumentParser(description='Ziplineæ•°æ®æ‘„å…¥å·¥å…·')
    parser.add_argument('--bundle', default='custom_bundle', help='Bundleåç§°')
    parser.add_argument('--symbols', help='è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œé€—å·åˆ†éš”')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°æ‘„å…¥')
    parser.add_argument('--prepare-only', action='store_true', help='åªå‡†å¤‡æ•°æ®ï¼Œä¸æ‘„å…¥')
    parser.add_argument('--verify-only', action='store_true', help='åªéªŒè¯Bundle')
    
    args = parser.parse_args()
    
    print("æ•°æ®æ‘„å…¥å·¥å…·")
    print("=" * 40)
    
    # è§£æè‚¡ç¥¨ä»£ç 
    symbols = None
    if args.symbols:
        symbols = [s.strip() for s in args.symbols.split(',')]
        print(f"æŒ‡å®šè‚¡ç¥¨: {symbols}")
    
    # è®¾ç½®ç¯å¢ƒ
    zipline_root = setup_zipline_environment()
    
    if args.verify_only:
        # åªéªŒè¯Bundle
        success = verify_bundle(args.bundle)
        return 0 if success else 1
    
    # å‡†å¤‡æ•°æ®
    print("\næ­¥éª¤1: å‡†å¤‡æ‘„å…¥æ•°æ®")
    data_ready, data_dir = prepare_data_for_ingest(symbols)
    
    if not data_ready:
        print("æ•°æ®å‡†å¤‡å¤±è´¥")
        return 1
    
    if args.prepare_only:
        print("æ•°æ®å‡†å¤‡å®Œæˆï¼Œè·³è¿‡æ‘„å…¥æ­¥éª¤")
        return 0
    
    # åˆ›å»ºBundleå®šä¹‰
    print("\næ­¥éª¤2: åˆ›å»ºBundleå®šä¹‰")
    bundle_file = create_bundle_definition(args.bundle, data_dir, symbols)
    
    # æ‰§è¡Œæ‘„å…¥
    print("\næ­¥éª¤3: æ‰§è¡Œæ•°æ®æ‘„å…¥")
    ingest_success = run_zipline_ingest(args.bundle, args.force)
    
    if not ingest_success:
        print("æ•°æ®æ‘„å…¥å¤±è´¥")
        return 1
    
    # éªŒè¯ç»“æœ
    print("\næ­¥éª¤4: éªŒè¯æ‘„å…¥ç»“æœ")
    verify_success = verify_bundle(args.bundle)
    
    if verify_success:
        print(f"\næ‘„å…¥å®Œæˆ! Bundle '{args.bundle}' å·²å‡†å¤‡å°±ç»ª")
        print(f"ç°åœ¨å¯ä»¥åœ¨Ziplineç­–ç•¥ä¸­ä½¿ç”¨æ­¤Bundleè¿›è¡Œå›æµ‹")
        return 0
    else:
        print("\nBundleéªŒè¯å¤±è´¥")
        return 1

if __name__ == "__main__":
    sys.exit(main())
'''

# =============================================================================
# test_zipline_data.py - æµ‹è¯•Ziplineæ•°æ®
# =============================================================================

TEST_ZIPLINE_DATA_SCRIPT = '''#!/usr/bin/env python3
"""
Ziplineæ•°æ®æµ‹è¯•å·¥å…·
æµ‹è¯•æ‘„å…¥çš„Ziplineæ•°æ®æ˜¯å¦æ­£å¸¸å¯ç”¨

ä½¿ç”¨æ–¹å¼:
python scripts/test_zipline_data.py --bundle custom_bundle
python scripts/test_zipline_data.py --bundle custom_bundle --symbol 000001.SZ
"""

import sys
import argparse
from pathlib import Path
from datetime import datetime
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

def test_bundle_availability(bundle_name):
    """æµ‹è¯•Bundleæ˜¯å¦å¯ç”¨"""
    try:
        from zipline.data import bundles
        
        # æ£€æŸ¥Bundleæ˜¯å¦å·²æ³¨å†Œ
        available_bundles = bundles.bundles.keys()
        
        if bundle_name not in available_bundles:
            print(f"Bundle '{bundle_name}' æœªæ³¨å†Œ")
            print(f"å¯ç”¨çš„Bundle: {list(available_bundles)}")
            return False
        
        print(f"Bundle '{bundle_name}' å·²æ³¨å†Œ")
        return True
        
    except ImportError:
        print("Ziplineæœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥")
        return False
    except Exception as e:
        print(f"Bundleå¯ç”¨æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False

def load_bundle_data(bundle_name):
    """åŠ è½½Bundleæ•°æ®"""
    try:
        from zipline.data import bundles
        
        # åŠ è½½Bundle
        bundle_data = bundles.load(bundle_name)
        
        print(f"Bundleæ•°æ®åŠ è½½æˆåŠŸ")
        return bundle_data
        
    except Exception as e:
        print(f"Bundleæ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

def test_asset_finder(bundle_data):
    """æµ‹è¯•èµ„äº§æŸ¥æ‰¾å™¨"""
    try:
        asset_finder = bundle_data.asset_finder
        
        # è·å–æ‰€æœ‰èµ„äº§
        all_sids = asset_finder.sids
        assets = asset_finder.retrieve_all(all_sids)
        
        print(f"æ€»èµ„äº§æ•°é‡: {len(assets)}")
        
        if assets:
            print("å‰5ä¸ªèµ„äº§:")
            for asset in assets[:5]:
                print(f"  {asset.symbol}: {asset.start_date} ~ {asset.end_date}")
        
        return assets
        
    except Exception as e:
        print(f"èµ„äº§æŸ¥æ‰¾å™¨æµ‹è¯•å¤±è´¥: {e}")
        return []

def test_price_data(bundle_data, symbol=None):
    """æµ‹è¯•ä»·æ ¼æ•°æ®"""
    try:
        asset_finder = bundle_data.asset_finder
        equity_daily_bar_reader = bundle_data.equity_daily_bar_reader
        
        # é€‰æ‹©æµ‹è¯•èµ„äº§
        if symbol:
            try:
                asset = asset_finder.lookup_symbol(symbol, as_of_date=None)
            except:
                print(f"æ‰¾ä¸åˆ°è‚¡ç¥¨: {symbol}")
                return False
        else:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªèµ„äº§
            all_sids = asset_finder.sids
            if not all_sids:
                print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•èµ„äº§")
                return False
            asset = asset_finder.retrieve_asset(all_sids[0])
        
        print(f"æµ‹è¯•è‚¡ç¥¨: {asset.symbol}")
        
        # è·å–ä»·æ ¼æ•°æ®
        try:
            # è·å–æ•°æ®çš„æ—¥æœŸèŒƒå›´
            start_date = asset.start_date
            end_date = asset.end_date
            
            print(f"æ•°æ®èŒƒå›´: {start_date} ~ {end_date}")
            
            # è¯»å–ä»·æ ¼æ•°æ®
            fields = ['open', 'high', 'low', 'close', 'volume']
            
            for field in fields:
                try:
                    data = equity_daily_bar_reader.load_raw_arrays(
                        [field], start_date, end_date, [asset.sid]
                    )
                    
                    field_data = data[0][field].flatten()  # [assets, dates] -> [dates]
                    
                    # ç»Ÿè®¡ä¿¡æ¯
                    valid_count = (~pd.isna(field_data)).sum()
                    total_count = len(field_data)
                    
                    print(f"  {field}: {valid_count}/{total_count} æœ‰æ•ˆæ•°æ®ç‚¹")
                    
                    if valid_count > 0:
                        non_nan_data = field_data[~pd.isna(field_data)]
                        print(f"    èŒƒå›´: {non_nan_data.min():.4f} ~ {non_nan_data.max():.4f}")
                        print(f"    å‡å€¼: {non_nan_data.mean():.4f}")
                    
                except Exception as e:
                    print(f"  {field}: æ•°æ®è¯»å–å¤±è´¥ - {e}")
            
            return True
            
        except Exception as e:
            print(f"ä»·æ ¼æ•°æ®è¯»å–å¤±è´¥: {e}")
            return False
        
    except Exception as e:
        print(f"ä»·æ ¼æ•°æ®æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_calendar_alignment(bundle_data):
    """æµ‹è¯•äº¤æ˜“æ—¥å†å¯¹é½"""
    try:
        from zipline.utils.calendars import get_calendar
        
        # è·å–äº¤æ˜“æ—¥å†
        calendar = get_calendar('XSHG')  # ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€
        
        # è·å–2024å¹´çš„äº¤æ˜“æ—¥
        sessions_2024 = calendar.sessions_in_range('2024-01-01', '2024-12-31')
        
        print(f"2024å¹´äº¤æ˜“æ—¥æ•°é‡: {len(sessions_2024)}")
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸äº¤æ˜“æ—¥å†å¯¹é½
        asset_finder = bundle_data.asset_finder
        all_sids = asset_finder.sids
        
        if all_sids:
            first_asset = asset_finder.retrieve_asset(all_sids[0])
            
            # è·å–èµ„äº§çš„æ•°æ®æ—¥æœŸèŒƒå›´
            start_date = max(first_asset.start_date, sessions_2024[0].date())
            end_date = min(first_asset.end_date, sessions_2024[-1].date())
            
            # è®¡ç®—æœŸé—´çš„äº¤æ˜“æ—¥
            period_sessions = calendar.sessions_in_range(
                pd.Timestamp(start_date),
                pd.Timestamp(end_date)
            )
            
            print(f"æµ‹è¯•æœŸé—´äº¤æ˜“æ—¥: {len(period_sessions)}")
            print(f"æ•°æ®æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
            
            return True
        else:
            print("æ²¡æœ‰èµ„äº§å¯ä¾›æµ‹è¯•")
            return False
        
    except Exception as e:
        print(f"äº¤æ˜“æ—¥å†æµ‹è¯•å¤±è´¥: {e}")
        return False

def run_simple_backtest(bundle_name, test_symbol=None):
    """è¿è¡Œç®€å•å›æµ‹æµ‹è¯•"""
    try:
        from zipline import run_algorithm
        from zipline.api import order_percent, symbol, get_datetime
        
        # å®šä¹‰ç®€å•ç­–ç•¥
        def initialize(context):
            if test_symbol:
                context.asset = symbol(test_symbol)
            else:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨èµ„äº§
                context.asset = symbol('000001.SZ')  # é»˜è®¤
        
        def handle_data(context, data):
            # ç®€å•ä¹°å…¥æŒæœ‰ç­–ç•¥
            if data.can_trade(context.asset):
                if not context.portfolio.positions[context.asset]:
                    order_percent(context.asset, 1.0)
        
        # è¿è¡Œå›æµ‹
        print(f"è¿è¡Œç®€å•å›æµ‹æµ‹è¯•...")
        
        result = run_algorithm(
            start=pd.Timestamp('2024-01-01', tz='utc'),
            end=pd.Timestamp('2024-01-31', tz='utc'),  # çŸ­æœŸæµ‹è¯•
            initialize=initialize,
            handle_data=handle_data,
            bundle=bundle_name,
            capital_base=100000
        )
        
        if result is not None and not result.empty:
            final_value = result['portfolio_value'].iloc[-1]
            total_return = (final_value / 100000 - 1) * 100
            
            print(f"å›æµ‹å®Œæˆ!")
            print(f"  æœŸæœ«èµ„äº§: {final_value:,.2f}")
            print(f"  æ€»æ”¶ç›Šç‡: {total_return:.2f}%")
            return True
        else:
            print("å›æµ‹å¤±è´¥: è¿”å›ç©ºç»“æœ")
            return False
        
    except Exception as e:
        print(f"å›æµ‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='Ziplineæ•°æ®æµ‹è¯•å·¥å…·')
    parser.add_argument('--bundle', default='custom_bundle', help='Bundleåç§°')
    parser.add_argument('--symbol', help='æµ‹è¯•è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--skip-backtest', action='store_true', help='è·³è¿‡å›æµ‹æµ‹è¯•')
    
    args = parser.parse_args()
    
    print("Ziplineæ•°æ®æµ‹è¯•å·¥å…·")
    print("=" * 40)
    print(f"æµ‹è¯•Bundle: {args.bundle}")
    
    # æµ‹è¯•1: Bundleå¯ç”¨æ€§
    print("\næµ‹è¯•1: Bundleå¯ç”¨æ€§")
    if not test_bundle_availability(args.bundle):
        print("Bundleä¸å¯ç”¨ï¼Œé€€å‡ºæµ‹è¯•")
        return 1
    
    # æµ‹è¯•2: åŠ è½½Bundleæ•°æ®
    print("\næµ‹è¯•2: åŠ è½½Bundleæ•°æ®")
    bundle_data = load_bundle_data(args.bundle)
    
    if not bundle_data:
        print("Bundleæ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
        return 1
    
    # æµ‹è¯•3: èµ„äº§æŸ¥æ‰¾å™¨
    print("\næµ‹è¯•3: èµ„äº§æŸ¥æ‰¾å™¨")
    assets = test_asset_finder(bundle_data)
    
    if not assets:
        print("æ²¡æœ‰æ‰¾åˆ°èµ„äº§ï¼Œé€€å‡ºæµ‹è¯•")
        return 1
    
    # æµ‹è¯•4: ä»·æ ¼æ•°æ®
    print("\næµ‹è¯•4: ä»·æ ¼æ•°æ®")
    price_test_success = test_price_data(bundle_data, args.symbol)
    
    if not price_test_success:
        print("ä»·æ ¼æ•°æ®æµ‹è¯•å¤±è´¥")
    
    # æµ‹è¯•5: äº¤æ˜“æ—¥å†
    print("\næµ‹è¯•5: äº¤æ˜“æ—¥å†å¯¹é½")
    calendar_test_success = test_calendar_alignment(bundle_data)
    
    if not calendar_test_success:
        print("äº¤æ˜“æ—¥å†æµ‹è¯•å¤±è´¥")
    
    # æµ‹è¯•6: ç®€å•å›æµ‹ï¼ˆå¯é€‰ï¼‰
    if not args.skip_backtest:
        print("\næµ‹è¯•6: ç®€å•å›æµ‹")
        backtest_success = run_simple_backtest(args.bundle, args.symbol)
        
        if not backtest_success:
            print("å›æµ‹æµ‹è¯•å¤±è´¥")
    
    # æ€»ç»“
    print("\næµ‹è¯•æ€»ç»“:")
    print("=" * 40)
    
    test_results = [
        ("Bundleå¯ç”¨æ€§", True),
        ("æ•°æ®åŠ è½½", bundle_data is not None),
        ("èµ„äº§æŸ¥æ‰¾å™¨", len(assets) > 0),
        ("ä»·æ ¼æ•°æ®", price_test_success),
        ("äº¤æ˜“æ—¥å†", calendar_test_success),
    ]
    
    if not args.skip_backtest:
        test_results.append(("å›æµ‹åŠŸèƒ½", backtest_success))
    
    passed_count = sum(1 for _, passed in test_results if passed)
    total_count = len(test_results)
    
    for test_name, passed in test_results:
        status = "é€šè¿‡" if passed else "å¤±è´¥"
        icon = "âœ“" if passed else "âœ—"
        print(f"  {icon} {test_name}: {status}")
    
    print(f"\né€šè¿‡ç‡: {passed_count}/{total_count} ({passed_count/total_count*100:.1f}%)")
    
    if passed_count == total_count:
        print("\næ‰€æœ‰æµ‹è¯•é€šè¿‡! Ziplineæ•°æ®å‡†å¤‡å°±ç»ª")
        return 0
    else:
        print("\néƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®")
        return 1

if __name__ == "__main__":
    sys.exit(main())
'''

# =============================================================================
# åˆ›å»ºæ‰€æœ‰è„šæœ¬æ–‡ä»¶çš„ä¸»å‡½æ•°
# =============================================================================

def create_all_scripts():
    """åˆ›å»ºæ‰€æœ‰å·¥å…·è„šæœ¬"""
    scripts_to_create = [
        ('verify_token.py', VERIFY_TOKEN_SCRIPT),
        ('clear_cache.py', CLEAR_CACHE_SCRIPT),
        ('inspect_raw_data.py', INSPECT_RAW_DATA_SCRIPT),
        ('validate_data_format.py', VALIDATE_DATA_FORMAT_SCRIPT),
        ('verify_adjustment.py', VERIFY_ADJUSTMENT_SCRIPT),
        ('check_price_relationships.py', CHECK_PRICE_RELATIONSHIPS_SCRIPT),
        ('fix_price_relationships.py', FIX_PRICE_RELATIONSHIPS_SCRIPT),
        ('align_trading_calendar.py', ALIGN_TRADING_CALENDAR_SCRIPT),
        ('validate_zipline_format.py', VALIDATE_ZIPLINE_FORMAT_SCRIPT),
        ('zipline_ingest.py', ZIPLINE_INGEST_SCRIPT),
        ('test_zipline_data.py', TEST_ZIPLINE_DATA_SCRIPT),
    ]
    
    scripts_dir = PROJECT_ROOT / 'scripts'
    scripts_dir.mkdir(exist_ok=True)
    
    print("å®ç”¨å·¥å…·è„šæœ¬åˆ›å»ºå™¨")
    print("=" * 50)
    
    created_count = 0
    
    for filename, content in scripts_to_create:
        file_path = scripts_dir / filename
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            # è®¾ç½®æ‰§è¡Œæƒé™ (Unixç³»ç»Ÿ)
            try:
                import stat
                file_path.chmod(file_path.stat().st_mode | stat.S_IEXEC)
            except:
                pass
            
            print(f"åˆ›å»ºè„šæœ¬: {file_path}")
            created_count += 1
            
        except Exception as e:
            print(f"åˆ›å»ºè„šæœ¬å¤±è´¥ {filename}: {e}")
    
    print(f"\næˆåŠŸåˆ›å»º {created_count}/{len(scripts_to_create)} ä¸ªè„šæœ¬")
    
    # è¾“å‡ºä½¿ç”¨è¯´æ˜
    print(f"\nä½¿ç”¨è¯´æ˜:")
    print(f"  python scripts/verify_token.py --all")
    print(f"  python scripts/clear_cache.py --type all")
    print(f"  python scripts/inspect_raw_data.py --symbol 000001.SZ")
    print(f"  python scripts/validate_data_format.py --input data/raw/")
    print(f"  python scripts/verify_adjustment.py --symbol 000001.SZ --check-consistency")
    print(f"  python scripts/check_price_relationships.py --input data/processed/")
    print(f"  python scripts/fix_price_relationships.py --input data/raw/ --output data/cleaned/ --batch")
    print(f"  python scripts/align_trading_calendar.py --input data/raw/ --output data/aligned/")
    print(f"  python scripts/validate_zipline_format.py --input data/zipline/")
    print(f"  python scripts/zipline_ingest.py --bundle custom_bundle")
    print(f"  python scripts/test_zipline_data.py --bundle custom_bundle")
    
    return created_count

if __name__ == "__main__":
    create_all_scripts()